\chapter{Eigenwerte und Eigenvektoren \label{chapter:eigenwerte}}
\href{https://de.wikipedia.org/wiki/Eigenwertproblem}{Eigenwerte und Eigenvektoren} gehören zu den
wichtigsten Anwendungen der linearen Algebra: 
\begin{enumerate}
\item Die \href{http://de.wikipedia.org/wiki/Schrödingergleichung}{Schrödinger-Gleichung}
      \\[0.2cm]
      \hspace*{1.3cm}
      $\mathrm{H} \Psi = \mathrm{E} \cdot \Psi$
      \\[0.2cm]
      ist eine {\emph{\color{blue}Eigenwert-Gleichung}}.  Hier ist $\mathrm{H}$ der sogenannte
      \href{http://de.wikipedia.org/wiki/Hamilton-Operator}{Hamilton-Operator}, der auf die
      \href{http://de.wikipedia.org/wiki/Wellenfunktion}{Wellenfunktion} $\Psi$ angewendet wird.
      Dabei kommt wieder die Wellenfunktion $\Psi$ als Ergebnis heraus, allerdings multipliziert mit
      dem Eigenwert $\mathrm{E}$, der als die Energie der Wellenfunktion $\Psi$ interpretiert werden kann.

      Die Schrödinger-Gleichung ist die Grundlage der \href{http://de.wikipedia.org/wiki/Quantenmechanik}{Quanten-Mechanik}.
      Natürlich erwarte ich von Ihnen nicht, dass Sie diese Gleichung verstehen.  Ich habe diese
      Gleichung als erstes aufgelistet,  weil ich im Rahmen meiner Diplomarbeit im
      Wesentlichen nichts anderes gemacht habe, als auf numerischem Wege Schrödinger-Gleichungen zu
      lösen.  Dazu habe ich den Hamilton-Operator $\mathrm{H}$ zunächst durch eine Matrix
      approximiert.  Anschließend konnte ich die Eigenwerte dieser Matrix berechnen.  
\item In der Informatik werden Eigenvektoren unter anderem bei der 
      \href{https://de.wikipedia.org/wiki/Unabh%C3%A4ngigkeitsanalyse}{Unabhängigkeits-Analyse} 
      benötigt.  Die Unabhängigkeits-Analyse 
      kann beispielsweise dazu benutzt werden, das
      \href{http://research.ics.aalto.fi/ica/cocktail/cocktail_en.cgi}{Cocktail-Party-Problem} zu lösen.
      Bei diesem Problem geht es darum, verschiedene Geräuschquellen zu isolieren:  Stellen
      Sie sich vor, Sie sind auf einer Cocktail-Party.  Im Hintergrund spielt ein Klavier und Sie
      unterhalten sich mit einem Gesprächspartner.  Ihre Ohren hören sowohl das Klavier als auch den
      Gesprächspartner, aber solange Sie noch nicht zu viele Cocktails getrunken haben, ist Ihr
      Gehirn in der Lage, das, was Ihr Gesprächspartner Ihnen erzählt, aus 
      dem Gesamtgeräusch heraus zu filtern.  

      Neben dem Cocktail-Party-Problem gibt es zahlreiche anderen Anwendungen der
      Unabhängigkeits-Analyse sowohl in der Informatik im Bereich des 
      {\emph{\color{blue}Data-Minings}} als auch in vielen anderen Bereichen der Wissenschaft.
\item Eine andere Anwendung von Eigenvektoren ist die Lösung von
      {\emph{\color{blue}Rekurrenz-Gleichungen}}, die bei Komplexitätsanalyse von Algorithmen
      auftreten.  Dafür werden wir am Ende des Kapitels ein Beispiel geben.
\item Weitere Anwendungen von Eigenvektoren und den Ihnen zugeordneten Eigenwerten
      finden Sie beispielsweise unter 
      \\[0.2cm]
      \hspace*{1.3cm}
      \href{https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors#Applications}{
            https://en.wikipedia.org/wiki/Eigenvalues\_and\_eigenvectors\#Applications}.
\end{enumerate}
In diesem Kapitel führen wir zunächst Eigenwerte und Eigenvektoren ein und zeigen dann, wie sich 
lineare Rekurrenz-Gleichungen mit Hilfe von Eigenvektoren lösen lassen.  


\section{Definition und Berechnung von Eigenwerten}
\begin{Definition}[Eigenwert]
Es sei $A \in \mathbb{K}^{n \times n}$ eine quadratische Matrix.  Ein Vektor $\vec{x} \in \mathbb{K}^n$ mit $\vec{x} \not=\vec{0}$
ist ein {\emph{\color{blue}Eigenvektor}} der Matrix $A$ zum {\emph{\color{blue}Eigenwert}} $\lambda \in \mathbb{K}$ genau dann, wenn  
\\[0.2cm]
\hspace*{1.3cm}
$A \cdot \vec{x} = \lambda \cdot \vec{x}$
\\[0.2cm]
gilt.  \eoxs
\end{Definition}

\example
Die Matrix $A \in \mathbb{R}^{2 \times 2}$ sei als
\\[0.2cm]
\hspace*{1.3cm}
$A := \left(
  \begin{array}{ll}
    2 & 1 \\
    1 & 2
  \end{array}
  \right)
$
\\[0.2cm]
gegeben.  Die Vektoren $\vec{x}$ und $\vec{y}$ seien durch
\\[0.2cm]
\hspace*{1.3cm}
$\vec{x} := \left(
\begin{array}{r}
  1 \\
  1    
\end{array}\right)
$ \quad und \quad 
$\vec{y} := \left(
\begin{array}{r}
  1 \\
  -1    
\end{array}
\right)
$
\\[0.2cm]
definiert.  Dann gilt 
\\[0.2cm]
\hspace*{1.3cm}
$A \cdot \vec{x} = \left(
\begin{array}{r}
  3 \\
  3    
\end{array}\right) = 3 \cdot \vec{x}$ \quad und \quad
$A \cdot \vec{y} = \left(
\begin{array}{r}
  1 \\
  -1    
\end{array}\right) = 1 \cdot \vec{y}$.
\\[0.2cm]
Folglich ist $\vec{x}$ ein Eigenvektor von $A$ zum Eigenwert $3$, während $\vec{y}$ ein
Eigenvektor von $A$ zum Eigenwert $1$ ist.  \eox

\remark
Der Begriff des Eigenwerts und des Eigenvektors lässt sich auf beliebige lineare Operatoren
ausdehnen.  Ist $\mathrm{H} \in \mathcal{L}(V)$ ein Operator auf dem Vektor-Raum $V$, so ist
$\vec{x} \in V$ genau dann ein Eigenvektor zum Eigenwert $\lambda$, wenn 
\\[0.2cm]
\hspace*{1.3cm}
$\mathrm{H}(\vec{x}) = \lambda \cdot \vec{x}$
\\[0.2cm]
gilt.  Beispielsweise hat der Differential-Operator 
\\[0.2cm]
\hspace*{1.3cm}
$\mathrm{D}: \mathcal{C}^\infty(\mathbb{R}) \rightarrow  \mathcal{C}^\infty(\mathbb{R})$, 
\\[0.2cm]
der auf dem Raum 
\\[0.2cm]
\hspace*{1.3cm}
$\mathcal{C}^\infty(\mathbb{R}) := \bigl\{ f \in \mathbb{R}^\mathbb{R} \mid \mbox{$f$ ist beliebig oft  differenzierbar} \bigr\}$
\\[0.2cm]
durch die Gleichung
\\[0.2cm]
\hspace*{1.3cm}
$\mathrm{D}(f) := \displaystyle\frac{\mathrm{d}f}{\mathrm{d}x}$
\\[0.2cm]
definiert ist, für jedes $\lambda \in \mathbb{R}$ die Funktion
\\[0.2cm]
\hspace*{1.3cm}
 $x \mapsto e^x$ 
\\[0.2cm]
als Eigenvektor zum Eigenwert $\lambda$.  
Der Vektor-Raum $\mathcal{C}^\infty(\mathbb{R})$ der unendlich oft differenzierbaren Funktionen ist
unendlich-dimensional und hat daher eine wesentlich komplexere Struktur als die
endlich-dimensionalen Vektor-Räume, mit denen wir uns im Rest dieser Vorlesung beschäftigen werden.
Da bei endlich-dimensionalen Vektor-Räumen jede lineare Abbildung durch eine Matrix dargestellt
werden kann, verlieren wir nichts, wenn wir uns auf die Bestimmung von Eigenvektoren und Eigenwerten
von Matrizen beschränken. 
\eox

Der folgende Satz stellt einen Zusammenhang zwischen Eigenwerten und Determinanten her.  Dieser Satz
ist der Grund, warum wir gezwungen waren, den zugegebenermaßen sehr komplexen Begriff der
Determinanten im letzten Kapitel einzuführen.

\begin{Satz}
  Es sei $A \in \mathbb{K}^{n \times n}$.  Dann ist $\lambda \in \mathbb{K}$ genau dann ein
  Eigenwert von $A$, wenn 
  \\[0.2cm]
  \hspace*{1.3cm}
  $\mathtt{det}(\lambda \cdot \mathrm{E}_n - A) = 0$
  \\[0.2cm]
  gilt. 
\end{Satz}

\proof
Falls $\vec{x} \not= \vec{0}$ ein Eigenvektor von $A$ zum Eigenwert $\lambda$ ist, so haben wir die folgende
Kette von Äquivalenzen:
\\[0.2cm]
\hspace*{1.3cm}
$
\begin{array}[b]{lll}
                  & \exists \vec{x} \in \mathbb{K}^n: \bigl(\vec{x} \not=\vec{0} \wedge A \cdot \vec{x}  = \lambda \cdot \vec{x}\bigr) \\[0.2cm]
\Leftrightarrow   & \exists \vec{x} \in \mathbb{K}^n: \bigl(\vec{x} \not=\vec{0} \wedge A \cdot \vec{x} - \lambda \cdot \mathrm{E}_n \cdot \vec{x} = \vec{0}\bigr) \\[0.2cm]
\Leftrightarrow   & \exists \vec{x} \in \mathbb{K}^n: \bigl(\vec{x} \not=\vec{0} \wedge (A - \lambda \cdot \mathrm{E}_n) \cdot \vec{x} = \vec{0}\bigr) \\[0.2cm]
\Leftrightarrow   & \mathtt{Kern}(A - \lambda \cdot \mathrm{E}_n) \not= \{\vec{0}\} \\[0.2cm]
\Leftrightarrow   & \mbox{$A - \lambda \cdot \mathrm{E}_n$ ist nicht invertierbar} \\[0.2cm]
\Leftrightarrow   & \mathtt{det}(A - \lambda \cdot \mathrm{E}_n) = 0  \\[0.2cm]
\Leftrightarrow   & \mathtt{det}(\lambda \cdot \mathrm{E}_n - A) = 0  
\end{array}
$
\qed

Nach dem letzten Satz sind die Eigenwerte genau die Werte $\lambda$, für die der Ausdruck 
\\[0.2cm]
\hspace*{1.3cm}
 $\mathtt{det}(\lambda \cdot \mathrm{E}_n- A)$
\\[0.2cm]
den Wert $0$ annimmt.  Berechnen wir diesen Ausdruck mit Hilfe der von Leibniz angegebenen Formel,
so erhalten wir ein Polynom in der Unbestimmten $\lambda$.  Dieses Polynom heißt das
{\emph{\color{blue}charakteristische Polynom}} der Matrix $A$ und ist formal als
\\[0.2cm]
\hspace*{1.3cm}
$\chi_A(\lambda) = \mathtt{det}(\lambda \cdot \mathrm{E}_n - A)$
\\[0.2cm]
definiert.  Die Nullstellen von $\chi_A$ sind also gerade die Eigenwerte von $A$, formal gilt
\\[0.2cm]
\hspace*{1.3cm}
$\lambda$ ist Eigenwert von $A$ \quad g.d.w. \quad $\chi_A(\lambda) = 0$.  
\\[0.2cm]
Falls $A \in \mathbb{K}^{n \times n}$ ist, so ist $\chi_A(\lambda)$ ein Polynom vom Grad $n$ in der Unbestimmten $\lambda$.


\example
Die Matrix $A \in \mathbb{R}^{2 \times 2}$ sei wie oben als
\\[0.2cm]
\hspace*{1.3cm}
$A = \left(
  \begin{array}{ll}
    2 & 1 \\
    1 & 2
  \end{array}
  \right)
$
\\[0.2cm]
gegeben.  Dann kann das charakteristische Polynom $\chi_A(\lambda)$ wie folgt berechnet werden:
\\[0.2cm]
\hspace*{1.3cm}
$
\begin{array}[t]{lcl}
  \chi_A(\lambda) & = & \mathtt{det}(\lambda \cdot \mathrm{E}_n - A) \\[0.2cm]
                  & = & \mathtt{det}\left(\begin{array}{cc}
                                          \lambda - 2 & -1          \\
                                                    -1 & \lambda - 2
                                          \end{array}
                        \right) \\[0.4cm]
                  & = & (\lambda - 2) \cdot (\lambda - 2) - (-1) \cdot (-1) \\[0.2cm]
                  & = & \lambda^2 - 4 \cdot \lambda + 4 - 1 \\[0.2cm]
                  & = & \lambda^2 - 4 \cdot \lambda + 3 \\[0.2cm]
                  & = & (\lambda - 3) \cdot (\lambda - 1). 
\end{array}
$
\\[0.2cm]
Offenbar hat dieses Polynom die Nullstellen $\lambda = 3$ und $\lambda = 1$.  Um beispielsweise den
Eigenvektor zum Eigenwert $\lambda = 3$ zu berechnen, müssen wir die Gleichung
\\[0.2cm]
\hspace*{1.3cm}
$
\left(
  \begin{array}{ll}
    2 & 1 \\
    1 & 2 
  \end{array}
\right) \cdot \left(\begin{array}{l} x_1 \\ x_2 \end{array}\right) = 3 \cdot \left(\begin{array}{l} x_1 \\ x_2 \end{array}\right)
$
\\[0.2cm]
lösen.  Das liefert die beiden Gleichungen
\\[0.2cm]
\hspace*{1.8cm}
$2 \cdot x_1 + x_2 = 3 \cdot x_1$ \quad und \quad  $x_1 + 2 \cdot x_2 = 3 \cdot x_2$
\\
die wir zu
\\
\hspace*{1.8cm}
$-x_1 + x_2 = 0$ \quad und \quad  $x_1 - x_2 = 0$.
\\[0.2cm]
vereinfachen.
Offenbar ist die zweite Gleichung zur ersten Gleichung äquivalent und kann daher weggelassen
werden.  Da wir dann nur noch eine Gleichung aber zwei Unbekannte haben, können wir eine der
Unbekannten frei wählen. Wie müssen lediglich darauf achten, dass der Vektor
 $\ds\left(x_1 \atop x_2\right)$ vom Nullvektor verschieden ist.  Wir setzen daher $x_1 := 1$ und
 finden dann $x_2 = 1$.  Damit haben wir den Vektor $\ds\vec{x} = \left(1 \atop 1\right)$
als Eigenvektor der Matrix $A$ zum Eigenwert $3$ gefunden.

\exercise
Überlegen Sie sich, für welche Werte von $a$, $b$ und $c$ die Matrix 
\\[0.2cm]
\hspace*{1.3cm}
$A = \left(
  \begin{array}{ll}
    a & b \\
    b & c
  \end{array}
\right)
$
\\[0.2cm]
zwei verschiedene reelle Eigenwerte besitzt. \eox

\exercise
Für welche Werte von $\varphi$ hat die Matrix
\\[0.2cm]
\hspace*{1.3cm}
$A = \left(
  \begin{array}{rr}
    \cos(\varphi) & \sin(\varphi) \\
    -\sin(\varphi) & \cos(\varphi)
  \end{array}
\right)
$
\\[0.2cm]
keinen reellen Eigenwert? \eoxs

\begin{Definition}[Diagonalisierbarkeit] \hspace*{\fill} \\
Eine Matrix $A \in \mathbb{K}^{n \times n}$ ist {\emph{\color{blue}diagonalisierbar}} genau dann, wenn die Matrix
 $n$ linear unabhängige Eigenvektoren besitzt.  \eoxs 
\end{Definition}

\remark
Ist  $A \in \mathbb{K}^{n \times n}$ diagonalisierbar, so gibt es also $n$ verschiedene Vektoren
$\vec{x}_1, \cdots, \vec{x}_n$, so dass
\\[0.2cm]
\hspace*{1.3cm}
$A \cdot \vec{x}_i = \lambda_i \cdot \vec{x}_i$ \quad für alle $i=\{1,\cdots,n\}$
\\[0.2cm]
gilt. Fassen wir die $n$ Eigenvektoren $\vec{x}_i$ zu einer $n \times n$ Matrix $X$ zusammen, die
wir als
\\[0.2cm]
\hspace*{1.3cm}
$X = (\vec{x}_1, \cdots, \vec{x}_n)$
\\[0.2cm]
schreiben, wobei die $\vec{x}_i$ die Spalten der Matrix $X$ sind, so gilt
\\[0.2cm]
\hspace*{1.3cm}
$A \cdot X = (\lambda_1 \cdot \vec{x}_1, \cdots, \lambda_n \cdot \vec{x}_n) = X \cdot D$,
\\[0.2cm]
wobei wir die Matrix $D$ als Diagonal-Matrix definieren, deren Diagonal-Elemente die Eigenwerte
$\lambda_i$ sind, während alle anderen Einträge den Wert $0$ haben.  Damit gilt 
\\[0.2cm]
\hspace*{1.3cm}
$D = \left(
  \begin{array}{llllll}
    \lambda_1 & 0         & 0         & \cdots & 0 & 0 \\ 
    0         & \lambda_2 & 0         & \cdots & 0 & 0 \\
    0         & 0         & \lambda_3 & \cdots & 0 & 0 \\
    \vdots    & \vdots    & \vdots    & \ddots & \vdots & \vdots \\
    0         & 0         & 0         & \cdots  & \lambda_{n-1} & 0 \\
    0         & 0         & 0         & \cdots & 0 & \lambda_n 
  \end{array}
\right)
$.
\\[0.2cm]
In Komponentenschreibweise können wir die Matrix $D$ als
\\[0.2cm]
\hspace*{1.3cm}
$D = \bigl(\lambda_i \cdot \delta_{i,j}\bigr)$ \quad für alle $i,j\in \{1,\cdots,n\}$
\\[0.2cm]
schreiben, wobei $\delta_{i,j}$ das bereits früher definierte
\href{http://de.wikipedia.org/wiki/Kronecker-Delta}{Kronecker-Delta} bezeichnet.  Da die $n$
Vektoren $\vec{ x}_1, \cdots, \vec{x}_n$ linear unabhängig sind, ist 
die Matrix $X = (\vec{x}_1, \cdots, \vec{x}_n)$ invertierbar.  Damit können wir die Gleichung
\\[0.2cm]
\hspace*{1.3cm}
$A \cdot X = X \cdot D$
\\[0.2cm]
von rechts mit der Matrix $X^{-1}$ multiplizieren und erhalten
\\[0.2cm]
\hspace*{1.3cm}
$A = X \cdot D \cdot X^{-1}$.
\\[0.2cm]
Dies ist nützlich, wenn wir Potenzen der Matrix $A$ bilden wollen.  Beispielsweise gilt
\\[0.2cm]
\hspace*{1.3cm}
$A^2 = X \cdot D \cdot X^{-1} \cdot X \cdot D \cdot X^{-1} = X \cdot D^2 \cdot X^{-1}$
\\[0.2cm]
und durch eine einfache Induktion nach $k$ können wir nach demselben Prinzip zeigen, dass
\\[0.2cm]
\hspace*{1.3cm}
$A^k = X \cdot D^k \cdot X^{-1}$ \quad für alle $k \in \mathbb{N}$
\\[0.2cm]
gilt.  Diese Beobachtung hilft uns bei der Berechnung der Potenzen $A^k$, denn die Potenzen der
Diagonal-Matrix $D$ sind wesentlich  leichter zu berechnen sind als die Potenzen von $A$, es gilt
\\[0.2cm]
\hspace*{1.3cm}
$D^k = \left(
  \begin{array}{llllll}
    \lambda_1^k & 0         & 0         & \cdots & 0 & 0 \\ 
    0           & \lambda_2^k & 0         & \cdots & 0 & 0 \\
    0           & 0         & \lambda_3^k & \cdots & 0 & 0 \\
    \vdots      & \vdots    & \vdots    & \ddots & \vdots & \vdots \\
    0           & 0         & 0         & \cdots  & \lambda_{n-1}^k & 0 \\
    0           & 0         & 0         & \cdots & 0 & \lambda_n^k 
  \end{array}
\right)
$.
\\[0.2cm]
Diese Beobachtung wird uns im nächsten Abschnitt die explizite Berechnung der
\href{http://de.wikipedia.org/wiki/Fibonacci-Folge}{\emph{Fibonacci-Zahlen}} ermöglichen. 

\section{Die Berechnung der Fibonacci-Zahlen}
Die Folge $(f_n)_{n\in\mathbb{N}}$ der \href{http://de.wikipedia.org/wiki/Fibonacci-Folge}{Fibonacci-Zahlen} ist rekursiv durch die Gleichung
\\[0.2cm]
\hspace*{1.3cm}
$f_{n+2} = f_{n+1} + f_n$
\\[0.2cm]
zusammen mit den Anfangs-Bedingungen $f_0 = 0$ und $f_1 = 1$ definiert.   Eine Gleichung der obigen Form
wird als {\emph{\color{blue}lineare Rekurrenz-Gleichung}} bezeichnet.  Solche Gleichungen werden uns im zweiten
Semester bei der Abschätzung der Komplexität von Algorithmen mehrfach begegnen und wir zeigen nun am Beispiel der
Fibonacci-Zahlen, wie sich eine solche Rekurrenz-Gleichung lösen lässt.  Dazu definieren wir
zunächst die Matrix $A$ als
\\[0.2cm]
\hspace*{1.3cm}
$A := \left(
  \begin{array}{ll}
    0 & 1 \\
    1 & 1 
  \end{array}
\right)
$.
\\
Weiter definieren wir den Vektor $\vec{x}_n$ für alle $n \in \mathbb{N}$ als 
$\ds\vec{x}_n := \left(f_n \atop f_{n+1}\right)$.  Dann haben wir
\\[0.2cm]
\hspace*{1.3cm}
$A \cdot \vec{x}_n = \left(
  \begin{array}{ll}
    0 & 1 \\
    1 & 1 
  \end{array}
\right) \cdot \left(
  \begin{array}{c}
    f_n \\ f_{n+1}
  \end{array} 
\right) = 
\left(
  \begin{array}{c}
     f_{n+1} \\ f_n + f_{n+1}
  \end{array} 
\right) = \left(
  \begin{array}{c}
     f_{n+1} \\ f_{n+2}
  \end{array} 
\right) = \vec{x}_{n+1}.
$
\\[0.2cm]
Wir haben also gezeigt, dass
\\[0.2cm]
\hspace*{1.3cm}
$\vec{x}_{n+1} = A \cdot \vec{x}_n$ \quad für alle $n \in \mathbb{N}$
\\[0.2cm]
gilt.  Aus dieser Gleichung $\vec{x}_{n+1} = A \cdot\vec{x}_n$ folgt durch eine einfache Induktion nach $n$, dass
\\[0.2cm]
\hspace*{1.3cm}
$\vec{x}_n = A^n \cdot\vec{x}_0$
\\[0.2cm]
gilt, wobei $A^0$ die Einheits-Matrix $\mathrm{E}_2$ ist und $\ds\vec{x}_0 = \left(0 \atop 1\right)$ gilt.
Wollen wir die Potenzen $A^n$ berechnen, so ist es zweckmäßig, die Matrix $A$ vorher zu diagonalisieren.  Wir
berechnen zunächst das charakteristische Polynom von $A$:
\\[0.2cm]
\hspace*{1.3cm}
$\chi_A(\lambda) = \mathtt{det}(\lambda \cdot \mathrm{E}_2 - A) 
 = \mathtt{det}\left(
   \begin{array}{rr}
     \lambda  &  -1 \\
     -1       & \lambda - 1
   \end{array}
   \right) = \lambda \cdot (\lambda - 1) - 1 = \lambda^2 - \lambda - 1
$.
\\[0.2cm]
Nun bestimmen wir die Nullstellen von $\chi_A(\lambda)$, denn das sind die Eigenwerte von $A$:
\\[0.2cm]
\hspace*{1.3cm}
$
\begin{array}[t]{lll}
                & \chi_A(\lambda) = 0 \\[0.2cm]
\Leftrightarrow & \lambda^2 - \lambda - 1 = 0 \\[0.2cm] 
\Leftrightarrow & \lambda^2 - \lambda     = 1 \\[0.2cm]
\Leftrightarrow & \lambda^2 - \lambda + \left(\frac{1}{2}\right)^2 = 1 + \frac{1}{4} 
                & \mbox{quadratische Ergänzung}  \\[0.4cm]
\Leftrightarrow & \ds\left(\lambda - \frac{1}{2}\right)^2 = \frac{5}{4} \\[0.4cm]
\Leftrightarrow & \ds\lambda = \frac{1}{2} + \frac{\sqrt{5}}{2} \;\vee\; \lambda = \frac{1}{2} - \frac{\sqrt{5}}{2} 
\end{array}
$
\\[0.2cm]
Wir definieren daher
\\[0.2cm]
\hspace*{1.3cm}
$\lambda_1 = \frac{1}{2}\cdot(1 + \sqrt{5})$ \quad und \quad $\lambda_2 = \frac{1}{2}\cdot(1 - \sqrt{5})$.
\\[0.2cm]
Als nächstes müssen wir die Eigenvektoren bestimmen, die diesen beiden Eigenwerten zugeordnet sind.
Bezeichnen wir den Eigenvektor, der $\lambda_1$ zugeordnet ist, als $\ds\vec{y} := \left(y_1 \atop y_2\right)$, 
so haben wir also
\\[0.2cm]
\hspace*{1.3cm}
$A \cdot \vec{y} = \lambda_1 \cdot \vec{y}$
\\[0.2cm]
und aus dieser Gleichung folgt, dass für die Komponenten $y_1$ und $y_2$ die Gleichungen
\\[0.2cm]
\hspace*{1.3cm}
$y_2 = \lambda_1 \cdot y_1$  \quad und \quad $y_1 + y_2 = \lambda_1 \cdot y_2$
\\[0.2cm]
gelten müssen.  Es lässt sich zeigen, dass die zweite Gleichung aus der ersten folgt.
Setzen wir $y_1 := 1$, so erhalten wir aus der ersten Gleichung $y_2 = \lambda_1$.
Damit hat der Eigenvektor $\vec{y}$ die Form
\\[0.2cm]
\hspace*{1.3cm}
$\ds\vec{y} = \left(1 \atop \lambda_1\right)$.
\\[0.2cm]
Auf analoge Weise finden wie für den zweiten Eigenvektor $\ds\vec{z} := \left(z_1 \atop z_2\right)$
das Ergebnis
\\[0.2cm]
\hspace*{1.3cm}
$\ds\vec{z} =  \left(1 \atop \lambda_2\right)$.
\\[0.2cm]
Damit hat die Matrix der Eigenvektoren die Form
\\[0.2cm]
\hspace*{1.3cm}
$X := \left(
  \begin{array}{ll}
            1 &         1 \\
    \lambda_1 & \lambda_2 
  \end{array}
  \right)
$.
\\[0.2cm]
Diese Matrix hat die Determinante $\mathtt{det}(X) = \lambda_2 - \lambda_1 = -\sqrt{5}$.
Wir hatten in einer Übungsaufgabe eine Formel zur Berechnung des Inversen einer beliebigen $2 \times 2$ Matrix
hergeleitet.  Nach dieser Formel ist das Inverse der Matrix $X$ durch
\\[0.2cm]
\hspace*{1.3cm}
$X^{-1} = \bruch{1}{\mathtt{det}(A)} \cdot \left( 
  \begin{array}{rr}
    \lambda_2 & -1 \\
    -\lambda_1 & 1
  \end{array}
\right) = \bruch{1}{\sqrt{5}} \cdot \left( 
  \begin{array}{rr}
    -\lambda_2 & 1 \\
    \lambda_1 & -1
  \end{array}
\right) 
$
\\[0.2cm]
gegeben.  Damit gilt also
\\[0.2cm]
\hspace*{1.3cm}
$
\begin{array}[t]{lcl}
 \vec{x}_n & =&  A^n \cdot \left(
   \begin{array}{l}
    0 \\ 1     
   \end{array}
\right) = 
 \left(
  \begin{array}{ll}
            1 &         1 \\
    \lambda_1 & \lambda_2 
  \end{array}
  \right) \cdot
 \left(
  \begin{array}{ll}
    \lambda_1^n &       0 \\
             0  & \lambda_2^n 
  \end{array}
  \right) \cdot \bruch{1}{\sqrt{5}} \cdot \left( 
  \begin{array}{rr}
    -\lambda_2 & 1 \\
    \lambda_1 & -1
  \end{array}
\right) \cdot \left(
  \begin{array}{l}
     0 \\ 1    
  \end{array}
\right) \\[0.4cm]
   & = & \bruch{1}{\sqrt{5}} \cdot
 \left(
  \begin{array}{ll}
            1 &         1 \\
    \lambda_1 & \lambda_2 
  \end{array}
  \right) \cdot
 \left(
  \begin{array}{ll}
    \lambda_1^n &       0 \\
             0  & \lambda_2^n 
  \end{array}
  \right) \cdot \left(
    \begin{array}{r}
     1 \\ -1      
    \end{array}
\right) \\[0.4cm]
   & = & \bruch{1}{\sqrt{5}} \cdot
 \left(
  \begin{array}{ll}
            1 &         1 \\
    \lambda_1 & \lambda_2 
  \end{array}
  \right) \cdot
 \left(
  \begin{array}{r}
    \lambda_1^n  \\
   -\lambda_2^n 
  \end{array} 
  \right) \\[0.4cm]
   & = & \bruch{1}{\sqrt{5}} \cdot
 \left(
  \begin{array}{c}
    \lambda_1^n - \lambda_2^n \\[0.2cm]
   \lambda_1^{n+1} - \lambda_2^{n+1} 
  \end{array} 
  \right) 
\end{array}
$
\\[0.2cm]
Zwischen dem Vektor  $\vec{x}_n$ und den Fibonacci-Zahlen $f_n$ besteht nach Definition von $\vec{x}_n$ der Zusammenhang
\\[-0.2cm]
\hspace*{1.3cm}
$\ds \vec{x}_n = \left(
  \begin{array}{cc}
    f_n \\ f_{n+1}
  \end{array}
  \right)
$.
\\[0.2cm]
Daher haben wir für die $n$-te
Fibonacci-Zahl $f_n$ die Gleichung 
\\[0.2cm]
\hspace*{1.3cm}
\colorbox{red}{\framebox{\colorbox{blue}{\framebox{\colorbox{yellow}{
$\ds f_n = \frac{1}{\sqrt{5}} \cdot (\lambda_1^n -\lambda_2^n)$ 
\quad mit \quad $\ds\lambda_1 = \frac{1}{2}\cdot \bigl(1 + \sqrt{5}\bigr)$ 
\quad und \quad $\ds\lambda_2 = \frac{1}{2}\cdot \bigl(1 - \sqrt{5}\bigr)$}}}}}
\\[0.2cm]
gefunden.

\exercise
Lösen Sie die Rekurrenz-Gleichung
\\[0.2cm]
\hspace*{1.3cm}
$a_{n+2} = 3 \cdot a_{n+1} - 2 \cdot a_n$ \quad für die Anfangs-Bedingungen $a_0 = 0$, $a_1 = 1$
\\[0.2cm]
mit Hilfe der in diesem Abschnitt vorgestellten Methode. \eox

\section{Berechnung von Eigenwerten in \textsc{SetlX}}
Die Sprache \textsc{SetlX} stellt die Funktionen \texttt{la\_eigenValues} und \texttt{la\_eigenVectors} zur
Verfügung, mit denen Sie sowohl die Eigenwerte als auch die Eigenvektoren einer quadratischen Matrix berechnen
können.  Ist $A$ eine quadratische Matrix, so berechnet der Ausdruck
\\[0.2cm]
\hspace*{1.3cm}
$\texttt{la\_eigenValues}(A)$
\\[0.2cm]
die Liste der Eigenwerte, während der Ausdruck 
\\[0.2cm]
\hspace*{1.3cm}
$\texttt{la\_eigenVectors}(A)$
\\[0.2cm]
die Liste der diesen Eigenwerten zugeordneten Eigenvektoren zurück gibt.  Definieren wir beispielsweise
\\[0.2cm]
\hspace*{1.3cm}
\texttt{A := << <<1 2>> <<2 1>> >>;}
\\[0.2cm]
so liefert der Ausdruck $\texttt{la\_eigenValues(A)}$ das Ergebnis
\\[0.2cm]
\hspace*{1.3cm}
\texttt{[-0.9999999999999996, 3.0]},
\\[0.2cm]
während der Ausdruck $\texttt{la\_eigenVectors(A)}$ die Liste
\\[0.2cm]
\hspace*{1.3cm}
\texttt{[<<0.7071067811865475 -0.7071067811865475>>,}\\
\hspace*{1.5cm}
\texttt{<<0.7071067811865475 \ 0.7071067811865475>>]}
\\[0.2cm]
zurück gibt.  Wie wir sehen, werden die Eigenvektoren so normiert, dass Sie die Länge 1 haben.

\section{Reflexion}
\begin{enumerate}
\item Wie sind Eigenwerte und Eigenvektoren einer quadratischen Matrix definiert?
\item Wie ist das charakteristische Polynom einer Matrix definiert?
\item Wie können Sie die Eigenwerte einer quadratischen Matrix berechnen?
\item Wie können Sie eine lineare Rekurrenz-Gleichung mit Hilfe von Eigenvektoren lösen?
\end{enumerate}
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "lineare-algebra"
%%% ispell-local-dictionary: "deutsch8"
%%% End: 
