\chapter{Determinanten}
Der Begriff der \href{https://de.wikipedia.org/wiki/Determinante}{Determinante} ist einer der
zentralen Begriff der linearen Algebra: Einerseits werden Determinanten bei der Berechnung von
\href{https://de.wikipedia.org/wiki/Eigenwertproblem}{Eigenwerten} zur Definition des
\href{https://de.wikipedia.org/wiki/Charakteristisches_Polynom}{charakteristischen Polynoms} benötigt,   
andererseits spielen Determinanten auch außerhalb der linearen Algebra eine Rolle, beispielsweise
bei der Berechnung mehrdimensionaler Integrale.  Um Determinanten einführen zu können, müssen
wir zunächst das \emph{\color{blue}Signum} einer Permutation 
definieren und dann zeigen, dass es sich bei dieser Funktion um einen 
\href{https://de.wikipedia.org/wiki/Gruppenhomomorphismus}{Gruppen-Homomorphismus} handelt.
Letzteres ist mit einem nicht unerheblichen technischen Aufwand verbunden. 
\renewcommand{\labelenumi}{(\alph{enumi})}
\renewcommand{\labelenumii}{\arabic{enumii}.}


\section{Permutationen und Transpositionen}
Wir erinnern an die Definition der \blue{Permutations-Gruppe} $\mathcal{S}_n$.
\\[0.2cm]
\hspace*{1.3cm}
$\mathcal{S}_n = \bigl\{ R \subseteq \{1,\cdots,n\}\times\{1,\cdots,n\} \bigm| \mbox{$R$ ist Permutation} \bigr\}$.
\\[0.2cm]
Ist $R \in \mathcal{S}_n$, so hat $R$ die Form
\\[0.2cm]
\hspace*{1.3cm}
$R = \bigl\{ \pair(1,x_1), \cdots, \pair(n,x_n) \bigr\}$ \quad mit $\{ x_1, \cdots, x_n \} =\{1,\cdots,n\}$.
\\[0.2cm]
Da $R$ durch die Angabe der Zahlen $x_1,\cdots,x_n$ vollständig bestimmt ist, vereinbaren wir, in
Zukunft der Kürze halber die oben angegebene Permutation $R$ auch als die Liste 
\\[0.2cm]
\hspace*{1.3cm}
$R \;\widehat{=}\; [x_1,\cdots,x_n]$
\\[0.2cm]
zu schreiben.  Weiterhin werden wir $R$ oft als Funktion auffassen und dann beispielsweise
\\[0.2cm]
\hspace*{1.3cm}
$R(k) = x_k$ \quad schreiben, falls \quad $\pair(k,x_k) \in R$ gilt.

\remark
Es gilt $\mathtt{card}(\mathcal{S}_n) = n!$, wobei der Ausdruck $n!$ (gelesen: \emph{\color{blue}$n$ Fakultät})
durch die Formel 
\\[0.2cm]
\hspace*{1.3cm}
$\ds n! := \prod\limits_{i=1}^n i = 1 \cdot 2 \cdot 3 \cdot \mbox{\dots} \cdot (n-1) \cdot n$ 
\\[0.2cm] 
definiert ist. \eoxs

\proof
Wir haben oben bereits gesehen, dass wir die Permutationen $R \in \mathcal{S}_n$ als Listen der Form
\\[0.2cm]
\hspace*{1.3cm}
$R = [x_1, \cdots, x_n]$ \quad mit $x_i \in \{1,\cdots,n\}$
\\[0.2cm]
schreiben können, wobei die $x_i$ paarweise verschieden sind.  Damit haben wir bei der Wahl von
$x_1$ insgesamt $n$ Möglichkeiten.  Bei der Wahl von $x_2$ bleiben uns aufgrund der Einschränkung
$x_2 \not= x_1$ noch $n-1$ Möglichkeiten.  Falls die Elemente $x_1, \cdots, x_{i-1}$ bereits
festgelegt sind, haben wir bei der Wahl von $x_i$ noch $n - (i - 1)$ Wahlmöglichkeiten.   Insgesamt
haben wir also
\\[0.2cm]
\hspace*{1.3cm}
$\ds \prod\limits_{i=1}^n \bigl(n - (i-1)\bigr) = n \cdot (n-1) \cdot \mbox{\dots} \cdot 2 \cdot 1 = n!$
\\[0.2cm]
Möglichkeiten bei der Wahl der Elemente $x_i$. \qed 

Der folgende Begriff des {\emph{\color{blue}Vorzeichens}} oder auch
{\emph{\color{blue}Signums}} einer Permutation ist zentral für die  
Definition der Determinante.  Für Permutationen definieren wir eine Funktion
\\[0.2cm]
\hspace*{1.3cm}
$\texttt{sgn}: \mathcal{S}_n \rightarrow \{-1, +1 \}$,
\\[0.2cm]
von der wir später zeigen werden, dass die Gleichung
\begin{equation}
  \label{eq:sgn-homomorph}
  \mbox{\colorbox{blue}{\framebox{\colorbox{yellow}{
$\texttt{sgn}(R_1 \circ R_2) = \texttt{sgn}(R_1) \cdot \texttt{sgn}(R_2)$}}}}
\end{equation}
für beliebige Permutationen $R_1,R_2 \in \mathcal{S}_n$ gilt,  die Funktion $\texttt{sgn}$ ist also
mit dem Produkt zweier Permutationen verträglich und damit ein \blue{Homomorphismus} der Gruppe
$\langle \mathcal{S}_n, \mathtt{id}_n, \circ \rangle$ auf die Gruppe $\langle \{-1,+1\}, 1, \cdot \rangle$.  Die Definition der Funktion $\texttt{sgn}$ selbst
ist nicht besonders aufwendig,  aber der Nachweis, dass es sich bei dieser Funktion um einen
Gruppen-Homomorphismus handelt, erfordert  mehr Zeit, als in einer einführenden Vorlesung zur
Verfügung steht.  Der Vollständigkeit halber ist der Beweis trotzdem in diesem Skript enthalten.

\begin{Definition}[\blue{Signum}]
  Für Permutationen $R \in \mathcal{S}_n$ definieren wir für alle natürlichen Zahlen $n$ eine Funktion
  \\[0.2cm]
  \hspace*{1.3cm}
  $\mathtt{\color{blue}inv} : \mathcal{S}_n \mapsto 2^{\mathbb{Z}_n^+ \times \mathbb{Z}_n^+}$,
  \\[0.2cm]
  welche jeder Permutation $R \in \mathcal{S}_n$ die Menge derjeniger Paare natürlicher Zahlen $k$ und $l$
  zuordnet, für die
  \\[0.2cm]
  \hspace*{1.3cm}
  $k < l$ \quad und \quad $R(k) > R(l)$
  \\[0.2cm]
  gilt.  Formal definieren wir
  \\[0.2cm]
  \hspace*{1.3cm}
  ${\color{blue}\texttt{inv}(R)} := \bigl\{ \pair(k,l) \in \mathbb{Z}_n^+ \times \mathbb{Z}_n^+ \mid k < l \wedge  R(k) > R(l) \bigr\}$.
  \\[0.2cm]
  Die Menge $\texttt{inv}(R)$ bezeichnen wir als die Menge der {\emph{\color{blue}Inversionen}} von $R$.
  Darauf aufbauend definieren wir das {\emph{\color{blue}Vorzeichen}} (oder auch
  {\emph{\color{blue}Signum}}) der Permutation 
  $R$ als 
  \\[0.2cm]
  \hspace*{1.3cm}
  $\blue{\texttt{sgn}(R)} := (-1)^{\mathtt{card}(\texttt{inv}(R))} = \left\{
   \begin{array}{ll}
     +1 & \mbox{falls  $\,\mathtt{card}\bigl(\texttt{inv}(R)\bigr) \,\texttt{\%}\, 2 = 0$;} \\[0.2cm]     
     -1 & \mbox{falls  $\,\mathtt{card}\bigl(\texttt{inv}(R)\bigr) \,\texttt{\%}\, 2 = 1$.} 
   \end{array}
   \right.
  $ %$
  \\[0.2cm]
  Das Signum einer Permutation ist folglich $+1$, wenn die Anzahl der Inversionen von $R$ eine gerade
  Zahl ist, ansonsten hat das Signum den Wert $-1$.  \eox
\end{Definition}


Abbildung \ref{fig:signum.stlx} auf Seite \pageref{fig:signum.stlx} zeigt eine Implementierung der
Funktion \texttt{sgn} in der Sprache \textsc{SetlX}.  Die Permutationen werden in diesem Programm
als Listen dargestellt.  

\begin{figure}[!ht]
\centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  firstnumber   = 1,
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.0cm,
                  xrightmargin  = 0.0cm,
                ]
    inv := procedure(L) {
        n := #L;
        return { [k,l] : k in {1 .. n}, l in {k+1 .. n} | L[k] > L[l] };
    };
    signum := procedure(L) {
        return (-1) ** #inv(L);
    };
\end{Verbatim}
\vspace*{-0.3cm}
\caption{Berechnung des Signums einer Permutation.}
\label{fig:signum.stlx}
\end{figure}  %$

\exercise
Es sei $\mathtt{id}_n \in \mathcal{S}_n$ die identische Permutation aus $R_n$.  Stellen wir diese Permutation
durch eine Liste dar, so gilt also
\\[0.2cm]
\hspace*{1.3cm}
$\mathtt{id}_n = [1,2,3, \cdots, n]$.
\\[0.2cm]
Zeigen Sie, dass für alle natürlichen Zahlen $n$ die Gleichung
 $\texttt{sgn}(\mathtt{id}_n) = 1$
 gilt.  \eoxs


\exercise
Überlegen Sie, für welche Permutation $R \in \mathcal{S}_n$ die Kardinalität der Menge
$\texttt{inv}(R)$ maximal wird und berechnen Sie für diese Permutationen den Ausdruck
$\mathtt{card}(\texttt{inv}(R))$. \eox

Im Allgemeinen können Permutationen beliebig kompliziert sein.  Es lohnt sich, zunächst ganz
spezielle Permutationen zu betrachten, die nur zwei Elemente vertauschen.  Das führt zu der
folgenden Definition.

\begin{Definition}[\blue{Transposition}]
  Eine Permutation $R \in \mathcal{S}_n$ ist eine {\emph{\color{blue}Transposition}} genau dann, wenn
  nur zwei Elemente vertauscht werden.  Das ist äquivalent zu der Bedingung
  \\[0.2cm]
  \hspace*{1.3cm}
  $\mathtt{card}\bigl(\bigl\{ \pair(k,l) \in R \mid k \not= l \}\bigr) = 2$.
  \\[0.2cm]
  In Listen-Schreibweise können wir das folgendermaßen ausdrücken:
  \\[0.2cm]
  \hspace*{1.3cm}
  $R = [x_1, \cdots, x_n]$  ist Transposition \quad g.d.w. \quad $\mathtt{card}\bigl(\bigl\{i \in \{1,\cdots,n\} \mid x_i \not= i \bigr\}\bigr) = 2$.
  \\[0.2cm]
  Da eine Transposition $R$ durch die Angabe der beiden Elemente, die vertauscht werden, vollständig
  bestimmt wird, schreiben wir
  \\[0.2cm]
  \hspace*{1.3cm}
  $R \;\widehat{=}\; \pair(k, l)$ 
  \\[0.2cm]
  falls $k < l$ ist und $R$ die Transposition ist, welche die Zahlen $k$ und $l$ vertauscht.  In Listen-Schreibweise
  können wir diese Transposition auch als
  \\[0.2cm]
  \hspace*{1.3cm}
  $\pair(k, l) \;\widehat{=}\; [1, \cdots, k-1, l, k+1, \cdots, l-1, k, l+1, \cdots, n]$
  \\[0.2cm]
  schreiben.  Wollen wir die Transposition $\pair(k, l)$ als binäre Relation darstellen, so haben wir
  \\[0.2cm]
  \hspace*{1.3cm}
  $\pair(k,l) \;\widehat{=}\; \bigl\{ \pair(i,i) \mid i \in \{ 1, \cdots, n \} \backslash \{k,l\} \bigr\} \cup \bigl\{ \pair(k,l), \pair(l,k) \bigr\}$.
  \eoxs
\end{Definition}

\example
Die Permutation 
\\[0.2cm]
\hspace*{1.3cm}
$R = \bigl\{ \pair(1,1), \pair(2,3), \pair(3,2), \pair(4,4) \bigr\}$
\\[0.2cm]
ist eine Transposition und kann daher kürzer als
\\[0.2cm]
\hspace*{1.3cm}
$R \;\widehat{=}\; \pair(2,3)$
\\[0.2cm]
geschrieben werden. \eox

\remark
Im weiteren Verlauf werden wir zur Bezeichnung von Permutationen die kleinen griechische Buchstaben
$\mu$, $\varrho$, $\sigma$ und $\tau$ benutzen.
Schreiben wir eine Transposition $\tau$ in der Form $\tau = \pair(k,l)$, so ist a priori nicht
klar, für welche natürliche Zahl $n$ nun $\tau \in \mathcal{S}_n$ gilt.  Das ist bei der
Schreibweise $\tau = \pair(k,l)$ normalerweise kein Problem, denn einerseits geht meist aus dem Zusammenhang
klar hervor, welches $n \in  \mathbb{N}$ gemeint ist und andererseits ist es oft auch 
unwichtig, denn jede Permutation $\sigma \in \mathcal{S}_n$ kann in trivialer Weise zu einer Permutation
$\sigma'\in \mathcal{S}_m$ erweitert werden, falls $m \geq n$ gilt. \eox

Transpositionen sind einfacher als beliebige Permutationen und in der nächsten Aufgabe werden wir
sehen, dass sich  für eine Transpositionen $\tau$ das Vorzeichen $\texttt{sgn}(\tau)$ sofort berechnen lässt, denn es gilt
$\texttt{sgn}\bigl(\pair(k,l)\bigr) = -1$.

\exercise
Die Transposition $\tau$ habe die Form  $\tau = \pair(k,l)$ 
und es gelte $k < l$.  Berechnen Sie die Ausdrücke $\texttt{inv}(\tau)$ und $\texttt{sgn}(\tau)$.
\eox

\subsection{Die Homomorphie-Eigenschaft der Signum-Funktion$^*$}
Der Rest dieses Abschnitts beschäftigt sich mit dem Nachweis, dass die Funktion $\texttt{sgn}$ ein
Gruppen-Homomorphismus ist.  Die in der folgenden Aufgabe zu zeigende Eigenschaft wird später mehrfach
benötigt. 
\pagebreak

\exercise
Es seien $\mu,\sigma \in \mathcal{S}_n$ und es sei $k \in \mathbb{Z}_n^+$.  Zeigen Sie, dass gilt:
Gleichung 
\\[0.2cm]
\hspace*{1.3cm}
$(\mu \circ \sigma)(k) = \sigma\bigl(\mu(k)\bigr)$. \eox

Der nächste Satz zeigt uns, dass
sich alle nicht-trivialen Permutationen als Produkt von Transpositionen schreiben lassen.  

\begin{Satz} \label{satz:produkt-von-transpositionen}
  Ist $n \geq 2$ und gilt $\tau \in \mathcal{S}_n$, so lässt sich $\tau$ als Produkt von Transpositionen darstellen.
\end{Satz}

\proof
Wir beweisen diesen Satz durch vollständige Induktion nach $n$.
\begin{enumerate}
\item[I.A.:] $n = 2$.
           
             Verwenden wir die Listen-Schreibweise für Permutationen, so können wir $\mathcal{S}_2$ als
             \\[0.2cm]
             \hspace*{1.3cm}
             $\{ [1,2], [2,1] \}$
             \\[0.2cm]
             schreiben.  Die Permutation $[2,1]$ ist bereits eine Transposition und da 
             \\[0.2cm]
             \hspace*{1.3cm}
             $[1,2] = [2,1] \circ [2,1]$
             \\[0.2cm]
             gilt, lässt sich offenbar auch die einzige andere Permutation aus $\mathcal{S}_2$ als Produkt
             zweier Transpositionen schreiben. 
\item[I.S.:] $n \mapsto n + 1$.

             Wir definieren $k := \tau(n+1)$ und unterscheiden zwei Fälle.
             \renewcommand{\labelenumii}{\arabic{enumii}.}
             \begin{enumerate}
             \item Fall: $k = n+1$.
               
                   In diesem Fall enthält die Permutation $\tau$ also das Paar $\pair(n+1,n+1)$.  Dann
                   definieren wir
                   \\[0.2cm]
                   \hspace*{1.3cm}
                   $\tau' := \tau \backslash \{ \pair(n+1,n+1) \}$.
                   \\[0.2cm]
                   Damit ist $\tau' \in \mathcal{S}_n$ und lässt sich folglich nach Induktions-Voraussetzung als
                   Produkt von Transpositionen schreiben:
                   \\[0.2cm]
                   \hspace*{1.3cm}
                   $\tau' = \tau'_1 \circ \cdots \circ \tau'_m$ \quad mit $\tau'_i \in \mathcal{S}_n$.
                   \\[0.2cm]
                   Erweitern wir die Transpositionen $\tau'_i \in \mathcal{S}_n$ zu Transpositionen
                   $\tau_i \in \mathcal{S}_{n+1}$ indem wir
                   \\[0.2cm]
                   \hspace*{1.3cm}
                   $\tau_i := \tau'_i \cup \{ \pair(n+1, n+1) \}$
                   \\[0.2cm]
                   definieren, so erhalten wir
                   \\[0.2cm]
                   \hspace*{1.3cm}
                   $\tau = \tau_1 \circ \cdots \circ \tau_m$.
             \item Fall:  $k \leq n$.                   
               
                   In diesem Fall definieren wir eine Permutation $\tau'$ als
                   \\[0.2cm]
                   \hspace*{1.3cm}
                   $\tau' := \tau \circ \pair(k, n+1)$.
                   \\[0.2cm]
                   Wir berechnen den Wert von $\tau'(n+1)$:  Es gilt
                   \\[0.2cm]
                   \hspace*{1.3cm}
                   $\tau'(n+1) = \pair(k, n+1)\bigl(\tau(n+1)\bigr) = \pair(k, n+1)(k) = n+1$.
                   \\[0.2cm]
                   Damit bildet $\tau'$ also die Zahl $n+1$ auf sich selbst ab und folglich erfüllt
                   $\tau'$ die Voraussetzung des ersten Falls.  Da wir die Gültigkeit unserer Behauptung für
                   den ersten Fall bereits bewiesen haben, finden wir also Transpositionen 
                   $\tau_1, \cdots, \tau_m$, so dass
                   \\[0.2cm]
                   \hspace*{1.3cm}
                   $\tau' = \tau_1 \circ \cdots \circ \tau_m$
                   \\[0.2cm]
                   gilt.  Setzen wir hier die Definition von $\tau'$ ein, so erhalten wir
                   \\[0.2cm]
                   \hspace*{1.3cm}
                   $\tau \circ \pair(k, n+1) =  \tau_1 \circ \cdots \circ \tau_m$.
                   \\[0.2cm]
                   Da jede Transposition bezüglich des relationalen Produktes ihr eigenes Inverses
                   ist, können wir diese Gleichung von rechts mit $\pair(k, n+1)$ multiplizieren um
                   \\[0.2cm]
                   \hspace*{1.3cm}
                   $\tau =  \tau_1 \circ \cdots \circ \tau_m \circ \pair(k, n+1)$
                   \\[0.2cm]
                   zu erhalten.  Damit haben wir $\tau$ als Produkt von Transpositionen dargestellt.
                   \qed
             \end{enumerate}
             \renewcommand{\labelenumi}{(\alph{enumii})}
\end{enumerate}

\begin{Definition}[Elementare Transposition] \hspace*{\fill} \\
  Eine Transposition $\tau$ ist eine \emph{\color{blue}elementare Transposition} genau dann, wenn es ein 
  $k \in\mathbb{N}$ gibt, so dass 
  \\[0.2cm]
  \hspace*{1.3cm}
  $\tau = \pair(k, k+1)$
  \\[0.2cm]
  gilt.  Eine elementare Transposition vertauscht also \blue{benachbarte} Indizes.  \eoxs
\end{Definition}

\remark
Ist $\tau = \pair(k, k+1)$ eine elementare Transposition, so gilt
  \\[0.2cm]
  \hspace*{1.3cm}
  $\texttt{inv}(\tau) = \bigl\{ \pair(k, k+1) \bigr\}$
  \\[0.2cm]
  und daraus folgt sofort
  $\texttt{sgn}(\pair(k, k+1)) = -1$.
\eoxs

\begin{Lemma} \hspace*{\fill} \\
  Jede Transposition $\tau \in \mathcal{S}_n$ lässt sich als Produkt einer \blue{ungeraden} Anzahl elementarer Transpositionen
  darstellen.  
\end{Lemma}

\proof
Ist $k < l$, so gilt 
\\[0.2cm]
\hspace*{1.3cm}
$\pair(k,k+1) \circ \cdots \circ \pair(l-1,l) \circ \pair(l-1,l-2) \circ \cdots \circ \pair(k+1,k) = \pair(k,l)$.  
\\[0.2cm]
Diese Gleichung können Sie nachrechnen, wenn Sie die Permutation auf der linken Seite
auf alle $i \in \{1,\cdots,n\}$ anwenden.  Dazu ist lediglich eine Fallunterscheidung erforderlich,
bei der für $i$ die Fälle
\begin{enumerate}
\item $i < k$,
\item $i = k$,
\item $i > k \wedge i < l$,
\item $i = l$ und
\item $i > l$
\end{enumerate}
unterschieden werden.  Diese Fallunterscheidung tatsächlich auszuführen ist eine gute Übungsaufgabe.
Mit der obigen Gleichung haben wir $\pair(k,l)$ als Produkt elementarer Transpositionen dargestellt. 
Es bleibt zu zeigen, dass die Anzahl dieser elementaren Transpositionen insgesamt ungerade ist.  Die Anzahl der
elementaren Transpositionen in dem Produkt
\\[0.2cm]
\hspace*{1.3cm}
$\pair(k,k+1) \circ \cdots \circ \pair(l-1,l)$
\\[0.2cm]
beträgt $(l - 1) - k + 1 = l - k$, während das Produkt
\\[0.2cm]
\hspace*{1.3cm}
$\pair(l-2,l-1) \circ \cdots \circ \pair(k+1,k+2)$
\\[0.2cm]
aus $(l-2) - (k+1) + 1 = l - k -1$ Faktoren besteht. 
Insgesamt haben wir damit
\\[0.2cm]
\hspace*{1.3cm}
 $(l - k) + (l - k) - 1 = 2 \cdot (l-k) -1$ 
\\[0.2cm]
elementare Transpositionen zur
Darstellung von $\tau$ benötigt und diese Anzahl ist offenbar ungerade.
\qed

Das nächste Lemma zeigt uns, wie elementare Transpositionen auf Permutationen wirken.

\begin{Lemma}
  Falls die Permutation $\sigma \in \mathcal{S}_n$ in der Listen-Schreibweise die Form $\sigma = [x_1,\cdots,x_n]$ hat 
  und $\tau = \pair(k, k+1)$ eine elementare Transposition ist, dann gilt
  \\[0.2cm]
  \hspace*{1.3cm}
  $\tau \circ \sigma = \pair(k,k+1) \circ [x_1,\cdots,x_n] = [x_1, \cdots, x_{k-1},x_{k+1}, x_k,x_{k+2},\cdots, x_n]$.
  \\[0.2cm]
  Die Wirkung der Transposition $\tau$ auf die Permutation $\sigma$ besteht also darin, dass die Elemente
  $x_k$ und $x_{k+1}$ in der Liste, die $\sigma$ repräsentiert, ihre Positionen tauschen.
\end{Lemma}

\proof
Den Beweis der obigen Gleichung für $\tau \circ \sigma$ können
wir dadurch erbringen, dass wir die Permutation $\tau \circ \sigma$ auf die verschiedenen Elemente der
Menge $\{1,\cdots,n\}$ wirken lassen.  Wir führen dazu eine Fallunterscheidung durch.
\begin{enumerate}
\item Fall: $i \not\in \{k,k+1\}$. Dann haben wir
      \\[0.2cm]
      \hspace*{1.3cm}
      $(\tau \circ \sigma)(i) = \sigma\bigl(\tau(i)\bigr) = \sigma\bigl(\pair(k,k+1)(i)\bigr) = \sigma(i) = x_i$.
\item Fall: $i = k$.  Es gilt
      \\[0.2cm] 
      \hspace*{1.3cm}
      $(\tau \circ \sigma)(k) = \sigma\bigl(\tau(k)\bigr) = \sigma\bigl(\pair(k,k+1)(k)\bigr) = \sigma(k+1) = x_{k+1}$.
\item Fall: $i = k+1$.  Jetzt gilt
      \\[0.2cm] 
      \hspace*{1.3cm}
      $(\tau \circ \sigma)(k+1) = \sigma\bigl(\tau(k+1)\bigr) = \sigma\bigl(\pair(k,k+1)(k+1)\bigr) = \sigma(k) = x_k$.
      \qed
\end{enumerate}

Der Nachweis des folgenden Satzes stellt technisch gesehen die größte Hürde bei der Einführung der
Determinanten dar.  Ich habe mich für einen elementaren Beweis dieses Satzes entschieden: Dieser
Beweis ist zwar einerseits länger als die Argumentation, die in der Literatur sonst oft benutzt wird,
der Beweis ist aber dadurch, dass keine tiefliegenden Ideen verwendet werden, für den mathematisch
weniger geübten Studenten leichter nachvollziehbar als die entsprechenden Beweise, die beispielsweise in
\cite{fischer:2008} oder \cite{kowalsky:2003} zu finden sind.

\begin{Satz}
  Es sei $\sigma \in \mathcal{S}_n$ und $\tau = \pair(k, k + 1)$ sei eine elementare Transposition.  Dann gilt
  \\[0.2cm]
  \hspace*{1.3cm}
  $\texttt{sgn}(\tau \circ \sigma) = - \texttt{sgn}(\sigma)$.
\end{Satz}

\proof
Das Signum einer Permutation $\sigma$ wird mit Hilfe der Menge der Inversionen $\texttt{inv}(\sigma)$
berechnet, wobei die Menge $\texttt{inv}(\sigma)$ als
\\[0.2cm]
\hspace*{1.3cm}
$\texttt{inv}(\sigma) := \bigl\{ \pair(i,j) \in \mathbb{Z}_n^+ \times \mathbb{Z}_n^+ \mid i < j \wedge \sigma(i) > \sigma(j) \bigr\}$
\\[0.2cm]
definiert ist.  Zur Vereinfachung der Notation definieren wir (nur für diesen Beweis) die Menge
\\[0.2cm]
\hspace*{1.3cm}
$P := \bigl\{ \pair(i,j) \in \mathbb{Z}_n^+ \times \mathbb{Z}_n^+ \mid i < j \bigr\}$.
\\[0.2cm]
Damit schreibt sich die Menge $\texttt{inv}(\sigma)$ kürzer als
\\[0.2cm]
\hspace*{1.3cm}
$\texttt{inv}(\sigma) = \bigl\{ \pair(i,j) \in P \mid \sigma(i) > \sigma(j) \bigr\}$.
\\[0.2cm]
Wir müssen untersuchen, wie sich die Mengen $\texttt{inv}(\sigma)$ und $\texttt{inv}(\tau \circ \sigma)$
unterscheiden.  Dafür zerlegen wir die Menge $P$ in sechs Teile:
\\[0.2cm]
\hspace*{1.3cm}
$P := P_1 \uplus P_2 \uplus P_3 \uplus P_4 \uplus P_5 \uplus P_6$,
\\[0.2cm]
wobei das Zeichen $\uplus$ zum Ausdruck bringt, dass die Mengen $P_i$ paarweise disjunkt sind.
Diese Mengen werden wie folgt definiert:
\begin{enumerate}
\item $P_1 := \bigl\{ \pair(i,j) \in P \mid i,j \not\in \{k,k+1\} \bigr\}$,
\item $P_2 := \bigl\{ \pair(i,j) \in P \mid i = k \wedge j > k+1 \bigr\}$, 
\item $P_3 := \bigl\{ \pair(i,j) \in P \mid i = k+1 \wedge j > k+1 \bigr\}$, 
\item $P_4 := \bigl\{ \pair(i,j) \in P \mid i < k \wedge j = k \bigr\}$, 
\item $P_5 := \bigl\{ \pair(i,j) \in P \mid i < k \wedge j = k+1 \bigr\}$, 
\item $P_6 := \bigl\{ \pair(k,k+1) \bigr\}$.
\end{enumerate} 
Entsprechend der Zerlegung von $P$ können wir nun auch die Menge $\texttt{inv}(\sigma)$ in sechs
paarweise disjunkte Teile zerlegen.  Wir definieren
\\[0.2cm]
\hspace*{1.3cm}
$A_s := \bigl\{ \pair(i,j) \in P_s \mid \sigma(i) > \sigma(j) \bigr\}$ \quad für $s \in \{1,\cdots,6\}$.
\\[0.2cm]
Damit gilt
\\[0.2cm]
\hspace*{1.3cm}
$\texttt{inv}(\sigma) = A_1 \uplus A_2 \uplus A_3 \uplus A_4 \uplus A_5 \uplus A_6$. 
\\[0.2cm]
Zur Berechnung von $\texttt{inv}(\tau \circ \sigma)$ definieren wir entsprechend 
\\[0.2cm]
\hspace*{1.3cm}
$B_s := \bigl\{ \pair(i,j) \in P_s \mid \sigma(\tau(i)) > \sigma(\tau(j)) \bigr\}$ \quad für $s \in \{1,\cdots,6\}$.
\\[0.2cm]
Damit gilt offenbar
\\[0.2cm]
\hspace*{1.3cm}
$\texttt{inv}(\tau \circ \sigma) = B_1 \uplus B_2 \uplus B_3 \uplus B_4 \uplus B_5 \uplus B_6$. 
\\[0.2cm]
Als nächstes vergleichen wir die Mengen $A_s$ und $B_s$ für alle $s \in \{1,\cdots,6\}$.
Unser Ziel ist es, folgende Gleichungen nachzuweisen:
\begin{enumerate}
\item $\mathtt{card}(A_1) = \mathtt{card}(B_1)$,
\item $\mathtt{card}(A_2) = \mathtt{card}(B_3)$,
\item $\mathtt{card}(A_3) = \mathtt{card}(B_2)$,
\item $\mathtt{card}(A_4) = \mathtt{card}(B_5)$,
\item $\mathtt{card}(A_5) = \mathtt{card}(B_4)$,
\item $|\mathtt{card}(A_6) - \mathtt{card}(B_6)| = 1$.
\end{enumerate}
Daraus folgt dann, dass auch
\\[0.2cm]
\hspace*{1.3cm}
$\bigl|\mathtt{card}\bigl(\texttt{inv}(\sigma)\bigr)-\mathtt{card}\bigl(\texttt{inv}(\tau \circ \sigma)\bigr)\bigr| = 1$
\\[0.2cm]
und nach der Definition des Signums als 
\\[0.2cm]
\hspace*{1.3cm}
$\texttt{sgn}(\sigma) = (-1)^{\mathtt{card}(\texttt{inv}(\sigma))}$
\\[0.2cm]
folgt daraus, dass $\texttt{sgn}(\sigma)$ und $\texttt{sgn}(\tau \circ \sigma)$ verschiedenes Vorzeichen
haben, also die Behauptung.  Was bleibt ist der Nachweis der oben bezüglich der Kardinalitäten der
Mengen $A_i$ und $B_i$ aufgestellten Gleichungen.  Dieser Nachweis erfolgt durch eine Fallunterscheidung.
\begin{enumerate}
\item Falls $\pair(i,j) \in P_1$ ist, sind $i$ und $j$ sowohl von $k$ als auch von $k+1$
      verschieden.  Damit haben wir einerseits
      \\[0.2cm]
      \hspace*{1.3cm}
      $\sigma\bigl(\tau(i)\bigr) = \sigma\bigl(\pair(k,k+1)(i)\bigr) = \sigma(i)$ 
      \\[0.2cm]
      und andererseits
      \\[0.2cm]
      \hspace*{1.3cm}
      $\sigma\bigl(\tau(j)\bigr) = \sigma\bigl(\pair(k,k+1)(j)\bigr) = \sigma(j)$.
      \\[0.2cm]
      Also gilt
      \\[0.2cm]
      \hspace*{1.3cm}
      $(\tau \circ \sigma)(i) > (\tau \circ \sigma)(j) \;\Leftrightarrow\; \sigma(i) > \sigma(j)$.
      \\[0.2cm]
      Folglich ist $A_1 = B_1$ und daraus folgt, dass die Mengen $A_1$ und $B_1$ dieselbe
      Kardinalität haben:
      \\[0.2cm]
      \hspace*{1.3cm}
      $\mathtt{card}(A_1) = \mathtt{card}(B_1)$.
\item Einerseits haben wir
      \\[0.2cm]
      \hspace*{1.3cm}
      $
      \begin{array}[t]{lcl}
        A_2 & = &  \bigl\{ \pair(i,j) \in P_2 \mid \sigma(i) > \sigma(j) \bigr\} \\[0.2cm]
            & = &  \bigl\{ \pair(k,j) \in P \mid j > k + 1 \wedge \sigma(k) > \sigma(j) \bigr\}.
      \end{array}
      $
      \\[0.2cm]
      Andererseits gilt
      \\[0.2cm]
      \hspace*{1.3cm}
      $
      \begin{array}[t]{lcl}
        B_3 & = &  \bigl\{ \pair(i,j) \in P_3 \mid \sigma(\tau(i)) > \sigma(\tau(j)) \bigr\} \\[0.2cm]
            & = &  \bigl\{ \pair(k+1,j) \in P \mid j > k + 1 \wedge \sigma(\tau(k+1)) > \sigma(\tau(j)) \bigr\} \\[0.2cm]
            & = &  \bigl\{ \pair(k+1,j) \in P \mid j > k + 1 \wedge \sigma(k) > \sigma(j) \bigr\} 
      \end{array}
      $
      \\[0.2cm]
      Die Mengen $A_2$ und $B_3$ sind nicht gleich, denn in der ersten Komponente steht bei den
      Paaren aus $A_2$ die Zahl $k$, während dort bei den Paaren aus $B_3$ die Zahl $k+1$ steht. 
      Nichtsdestoweniger entspricht jedem Element aus $A_2$ genau ein Element aus $B_3$.  Damit haben
      $A_2$ und $B_3$ die gleiche Anzahl von Elementen:
      \\[0.2cm]
      \hspace*{1.3cm}
      $\mathtt{card}(A_2) = \mathtt{card}(B_3)$.
\item Einerseits haben wir
      \\[0.2cm]
      \hspace*{1.3cm}
      $
      \begin{array}[t]{lcl}
        A_3 & = &  \bigl\{ \pair(i,j) \in P_3 \mid \sigma(i) > \sigma(j) \bigr\} \\[0.2cm]
            & = &  \bigl\{ \pair(k+1,j) \in P \mid j > k + 1 \wedge \sigma(k+1) > \sigma(j) \bigr\}.
      \end{array}
      $
      \\[0.2cm]
      Andererseits gilt
      \\[0.2cm]
      \hspace*{1.3cm}
      $
      \begin{array}[t]{lcl}
        B_2 & = &  \bigl\{ \pair(i,j) \in P_2 \mid \sigma(\tau(i)) > \sigma(\tau(j)) \bigr\} \\[0.2cm]
            & = &  \bigl\{ \pair(k,j) \in P \mid j > k + 1 \wedge \sigma(\tau(k)) > \sigma(\tau(j)) \bigr\} \\[0.2cm]
            & = &  \bigl\{ \pair(k,j) \in P \mid j > k + 1 \wedge \sigma(k+1) > \sigma(j) \bigr\} 
      \end{array}
      $
      \\[0.2cm]
      Die Mengen $A_3$ und $B_2$ sind nicht gleich, denn in der ersten Komponente steht bei den
      Paaren aus $A_3$ die Zahl $k+1$, während dort bei den Paaren aus $B_2$ die Zahl $k$ steht. 
      Nichtsdestoweniger entspricht jedem Element aus $A_3$ genau ein Element aus $B_2$.  Damit gilt
      \\[0.2cm]
      \hspace*{1.3cm}
      $\mathtt{card}(A_3) = \mathtt{card}(B_2)$.
\item Einerseits haben wir
      \\[0.2cm]
      \hspace*{1.3cm}
      $
      \begin{array}[t]{lcl}
        A_4 & = &  \bigl\{ \pair(i,j) \in P_4 \mid \sigma(i) > \sigma(j) \bigr\} \\[0.2cm]
            & = &  \bigl\{ \pair(i,k) \in P \mid i < k \wedge \sigma(i) > \sigma(k) \bigr\}.
      \end{array}
      $
      \\[0.2cm]
      Andererseits gilt
      \\[0.2cm]
      \hspace*{1.3cm}
      $
      \begin{array}[t]{lcl}
        B_5 & = &  \bigl\{ \pair(i,j) \in P_5 \mid \sigma(\tau(i)) > \sigma(\tau(j)) \bigr\} \\[0.2cm]
            & = &  \bigl\{ \pair(i,k+1) \in P \mid i < k \wedge \sigma(\tau(i)) > \sigma(\tau(k+1)) \bigr\} \\[0.2cm]
            & = &  \bigl\{ \pair(i,k+1) \in P \mid i < k \wedge \sigma(i) > \sigma(k) \bigr\} 
      \end{array}
      $
      \\[0.2cm]
      Analog wie im letzten Fall sehen wir
      \\[0.2cm]
      \hspace*{1.3cm}
      $\mathtt{card}(A_4) = \mathtt{card}(B_5)$.
\item Einerseits haben wir
      \\[0.2cm]
      \hspace*{1.3cm}
      $
      \begin{array}[t]{lcl}
        A_5 & = &  \bigl\{ \pair(i,j) \in P_5 \mid \sigma(i) > \sigma(j) \bigr\} \\[0.2cm]
            & = &  \bigl\{ \pair(i,k+1) \in P \mid i < k \wedge \sigma(i) > \sigma(k+1) \bigr\}.
      \end{array}
      $
      \\[0.2cm]
      Andererseits gilt
      \\[0.2cm]
      \hspace*{1.3cm}
      $
      \begin{array}[t]{lcl}
        B_4 & = &  \bigl\{ \pair(i,j) \in P_4 \mid \sigma(\tau(i)) > \sigma(\tau(j)) \bigr\} \\[0.2cm]
            & = &  \bigl\{ \pair(i,k) \in P \mid i < k \wedge \sigma(\tau(i)) > \sigma(\tau(k)) \bigr\} \\[0.2cm]
            & = &  \bigl\{ \pair(i,k) \in P \mid i < k \wedge \sigma(i) > \sigma(k+1) \bigr\} 
      \end{array}
      $
      \\[0.2cm]
      Analog zum letzten Fall sehen wir
      \\[0.2cm]
      \hspace*{1.3cm}
      $\mathtt{card}(A_5) = \mathtt{card}(B_4)$.
\item Einerseits gilt 
      \\[0.2cm]
      \hspace*{1.3cm}
      $
      \begin{array}[t]{lcl}
        A_6 & = &  \bigl\{ \pair(i,j) \in P_6 \mid \sigma(i) > \sigma(j) \bigr\} \\[0.2cm]
            & = &  \bigl\{ \pair(k,k+1) \in P \mid \sigma(k) > \sigma(k+1) \bigr\}.
      \end{array}
      $
      \\[0.2cm]
      Andererseits haben wir
      \\[0.2cm]
      \hspace*{1.3cm}
      $
      \begin{array}[t]{lcl}
        B_6 & = &  \bigl\{ \pair(i,j) \in P_6 \mid \sigma(\tau(i)) > \sigma(\tau(j)) \bigr\} \\[0.2cm]
            & = &  \bigl\{ \pair(k,k+1) \in P \mid \sigma(\tau(k)) > \sigma(\tau(k+1)) \bigr\} \\[0.2cm]
            & = &  \bigl\{ \pair(k,k+1) \in P \mid \sigma(k+1) > \sigma(k) \bigr\}.
      \end{array}
      $
      \\[0.2cm]
      Da $\sigma$ eine Permutation ist, sind die Werte von $\sigma(k)$ und $\sigma(k+1)$ verschieden.  Folglich 
      gilt entweder $\sigma(k) > \sigma(k+1)$ oder $\sigma(k+1) > \sigma(k)$.  Damit ist dann  genau ein der beiden Mengen
      $A_6$ und $B_6$ leer, während die andere dass Paar $\pair(k,k+1)$ enthält, formal gilt
      \\[0.2cm]
      \hspace*{1.3cm}
      $|\mathtt{card}(A_6) - \mathtt{card}(B_6)| = 1$.  \qed
\end{enumerate}

\begin{Lemma}
  Es seien $\tau_1, \cdots, \tau_k \in \mathcal{S}_n$ elementare Transpositionen und es sei $\sigma \in \mathcal{S}_n$. 
  Dann gilt
  \\[0.2cm]
  \hspace*{1.3cm}
  $\texttt{sgn}(\tau_1 \circ \mbox{\dots} \circ \tau_k \circ \sigma) = (-1)^k \cdot \texttt{sgn}(\sigma)$.
\end{Lemma}

\proof
Der Beweis erfolgt durch Induktion über $k$.
\begin{enumerate}
\item[I.A.:] $k = 1$.  
  
             Hier folgt die Behauptung unmittelbar aus dem letzten Lemma.
\item[I.S.:] $k \mapsto k+1$.

             Es gilt
             \\[0.2cm]
             \hspace*{1.3cm}
             $
             \begin{array}[b]{lcl}
                 \texttt{sgn}(\tau_1 \circ \mbox{\dots} \circ \tau_{k+1} \circ \sigma)
                 & = & \texttt{sgn}\bigl(\tau_1 \circ (\tau_2 \circ \mbox{\dots} \circ \tau_{k+1} \circ \sigma)\bigr) \\[0.2cm]
                 & = & -\texttt{sgn}(\tau_2 \circ \mbox{\dots} \circ \tau_{k+1} \circ \sigma) \\[0.2cm]
                 & \stackrel{IV}{=} & - (-1)^k \cdot \texttt{sgn}(\sigma) \\[0.2cm]
                 & = & (-1)^{k+1} \cdot \texttt{sgn}(\sigma).  
             \end{array}
             $\qed
\end{enumerate}

\begin{Korollar}
  Ist $\tau \in \mathcal{S}_n$ eine Transposition und ist $\sigma \in \mathcal{S}_n$, so gilt
  \\[0.2cm]
  \hspace*{1.3cm}
  $\texttt{sgn}(\tau \circ \sigma) = -\texttt{sgn}(\sigma)$.
\end{Korollar}

\proof
Die Behauptung folgt aus dem letzten Lemma und der Tatsache, dass sich jede Transposition als
Produkt einer ungeraden Anzahl von elementaren Transpositionen darstellen lässt.  \qed

\begin{Korollar}
  Sind $\tau_1, \cdots, \tau_k \in \mathcal{S}_n$ Transpositionen, so gilt 
  \\[0.2cm]
  \hspace*{1.3cm}
  $\texttt{sgn}(\tau_1 \circ \mbox{\dots} \circ \tau_k) = (-1)^k$.
\end{Korollar}

\proof
Die Behauptung kann mit Hilfe des letzten Korollars durch eine Induktion nach $k$ gezeigt werden.
\begin{enumerate}
\item[I.A.:] $k=1$. Es gilt
             \\[0.2cm]
             \hspace*{1.3cm}
             $\texttt{sgn}(\tau) = \texttt{sgn}(\tau \circ \mathtt{id}_n) = -\texttt{sgn}(\mathtt{id}_n) = -1 = (-1)^1$.
\item[I.A.:] $k \mapsto k+1$. Wir haben
             \\[0.2cm]
             \hspace*{1.3cm}
             $
             \begin{array}[b]{lcl}
               \texttt{sgn}(\tau_1 \circ \tau_2 \circ \mbox{\dots} \circ \tau_{k+1}) 
               & = & - \texttt{sgn}(\tau_2 \circ \mbox{\dots} \circ \tau_{k+1}) \\[0.2cm]
               & \stackrel{IV}{=} & - (-1)^k \\[0.2cm]
               & = & (-1)^{k+1}.
             \end{array}
             $ \qed             
\end{enumerate}

Da sich jede Permutation $\sigma \in \mathcal{S}_n$ als Produkt von Transpositionen darstellen lässt,
sehen wir nun, dass das Vorzeichen $\texttt{sgn}(\sigma)$ genau dann den Wert $+1$ hat, wenn $\sigma$ sich
als Produkt einer geraden Anzahl von Permutationen darstellen lässt.  Andernfalls hat $\texttt{sgn}(\sigma)$
den Wert $-1$.  Damit können wir nun den folgenden Satz beweisen.

\begin{Satz} \label{satz:signum}
Es seien $\varrho_1, \varrho_2 \in \mathcal{S}_n$.  Dann gilt
\\[0.2cm]
\hspace*{1.3cm}
$\texttt{sgn}(\varrho_1 \circ \varrho_2) =\texttt{sgn}(\varrho_1) \circ \texttt{sgn}(\varrho_2)$.
\end{Satz}

\proof
Nach Satz \ref{satz:produkt-von-transpositionen} lassen sich sowohl $\varrho_1$ als auch $\varrho_2$ als
Produkte von Transpositionen darstellen.  Damit gibt es $k,l \in \mathbb{N}$ sowie
Transpositionen $\sigma_1, \cdots, \sigma_{k}, \tau_1, \cdots, \tau_l \in \mathcal{S}_n$, so dass
\\[0.2cm]
\hspace*{1.3cm}
$\varrho_1 = \sigma_1 \circ \mbox{\dots} \circ \sigma_{k}$ \quad und \quad
 $\varrho_2 = \tau_1 \circ \mbox{\dots} \circ \tau_l$
\\[0.2cm]
gilt.   Nach dem letzten Korollar haben wir dann: 
\begin{enumerate}
\item $\texttt{sgn}(\varrho_1) = \texttt{sgn}(\sigma_1 \circ \mbox{\dots} \circ \sigma_{k}) = (-1)^k$, 
\item $\texttt{sgn}(\varrho_2) = \texttt{sgn}(\tau_1 \circ \mbox{\dots} \circ \tau_l) = (-1)^l$,
\item $\texttt{sgn}(\varrho_1 \circ \varrho_2) = 
       \texttt{sgn}(\sigma_1 \circ \mbox{\dots} \circ \sigma_{k} \circ \tau_1 \circ \mbox{\dots} \circ \tau_l) = (-1)^{k+l}$.
\end{enumerate}
Also haben wir insgesamt
\\[0.2cm]
\hspace*{1.3cm}
$\texttt{sgn}(\varrho_1 \circ \varrho_2) = (-1)^{k+l} = (-1)^k \cdot (-1)^l = \texttt{sgn}(\varrho_1) \cdot \texttt{sgn}(\varrho_2)$.  
\qed

\section{Die Definition der Determinante nach Leibniz}
Wir haben nun alles Material zusammen um die \href{https://de.wikipedia.org/wiki/Determinante}{Determinante}
einer $n \times n$ Matrix so zu definieren, wie dies 1690 von
\href{http://de.wikipedia.org/wiki/Gottfried_Wilhelm_Leibniz}{Gottfried Wilhelm Leibniz\footnote{
  Den erfreuliche Umstand, dass der Name ``Leibniz'' selbst in bildungsfernen Schichten ein fester
  Begriff ist, haben wir   der Firma \href{http://www.bahlsen.com}{Bahlsen} zu verdanken, die ihre
  \href{http://en.wikipedia.org/wiki/Leibniz-Keks}{Butterkekse} nach diesem Mathematiker, Philosophen und
  Naturwissenschaftler benannt haben.
}} 
vorgeschlagen wurde.  Ist $A = (a_{i,j}) \in \mathbb{K}^{n \times n}$, wobei entweder 
$\mathbb{K}= \mathbb{R}$ oder $\mathbb{K}= \mathbb{C}$ gilt, so definieren wir die
{\emph{\color{blue}Determinante}} von $A$ (geschrieben $\mathtt{det}(A)$) über die Formel
\\[0.2cm]
\hspace*{1.3cm}
\colorbox{blue}{\framebox{\colorbox{yellow}{
$\ds\texttt{det}(A) :=  \sum\limits_{\sigma \in \mathcal{S}_n} \texttt{sgn}(\sigma) \cdot \prod\limits_{i=1}^n a_{i,\sigma(i)} 
  := \sum \Bigl\{ \texttt{sgn}(\sigma) \cdot  \prod\limits_{i=1}^n a_{i,\sigma(i)} \bigm| \sigma \in \mathcal{S}_n \Bigr\}
$.}}}

\example
Es sei $A$ eine $2 \times 2$ Matrix, es gilt also
\\[0.2cm]
\hspace*{1.3cm}
$A = \left(
  \begin{array}{ll}
    a_{1,1} & a_{1,2} \\[0.2cm] 
    a_{2,1} & a_{2,2} 
  \end{array}
  \right)
$
\\[0.2cm]
Zur Berechnung von $\texttt{det}(A)$ müssen wir uns überlegen, wie $\mathcal{S}_2$ aussieht.  Es gilt
\\[0.2cm]
\hspace*{1.3cm}
$\mathcal{S}_2 := \bigl\{ [1,2], [2,1] \bigr\}$.
\\[0.2cm]
Für die Permutation $[1,2]$ finden wir
\\[0.2cm]
\hspace*{1.3cm}
$\texttt{inv}([1,2]) = \{\}$ \quad und damit gilt \quad 
$\texttt{sgn}([1,2]) = (-1)^0 = +1$.
\\[0.2cm]
Für die  Permutation $[2,1]$ ergibt sich
\\[0.2cm]
\hspace*{1.3cm}
$\texttt{inv}([2,1]) = \{ \pair(1,2) \}$ \quad und damit gilt \quad
$\texttt{sgn}([2,1]) = (-1)^1 = -1$.
\\[0.2cm]
Insgesamt haben wir
\\[0.2cm]
\hspace*{0.5cm}
$\texttt{det}(A) = \texttt{det}\left(\begin{array}{ll}
    a_{1,1} & a_{1,2} \\[0.2cm] 
    a_{2,1} & a_{2,2} 
  \end{array}
  \right)
  = (+1) \cdot a_{1,1} \cdot a_{2,2} + (-1) \cdot a_{1,2} \cdot a_{2,1} 
  = a_{1,1} \cdot a_{2,2} - a_{1,2} \cdot a_{2,1} 
$
\eoxs

\example
Ist $A$ eine $3 \times 3$ Matrix, gilt also
\\[0.2cm]
\hspace*{1.3cm}
$A = \left(
  \begin{array}{lll}
    a_{1,1} & a_{1,2} & a_{1,3} \\[0.2cm] 
    a_{2,1} & a_{2,2} & a_{2,3} \\[0.2cm]
    a_{3,1} & a_{3,2} & a_{3,3} 
  \end{array}
  \right),
$
\\[0.2cm]
so finden wir für $\texttt{det}(A)$ die folgende Formel:
\\[0.2cm]
\hspace*{1.3cm}
$
\begin{array}{lcl}
  \texttt{det}(A) & = & a_{1,1} \cdot a_{2,2} \cdot a_{3,3}  \\
                  & + & a_{1,2} \cdot a_{2,3} \cdot a_{3,1}  \\
                  & + & a_{1,3} \cdot a_{2,1} \cdot a_{3,2}  \\
                  & - & a_{3,1} \cdot a_{2,2} \cdot a_{1,3}  \\
                  & - & a_{3,2} \cdot a_{2,3} \cdot a_{1,1}  \\
                  & - & a_{3,3} \cdot a_{2,1} \cdot a_{1,2}
\end{array}
$
\\[0.2cm]
Diese Formel wird auch als \href{https://de.wikipedia.org/wiki/Regel_von_Sarrus}{Regel von Sarrus} bezeichnet.
Abbildung \ref{fig:sarrus.eps} zeigt, wie Sie sich die Formel leicht merken können:  Dazu wiederholen Sie neben
der Matrix $A$ die ersten beiden Spalten von $A$.  Anschließend bilden Sie alle Produkte von Elementen der Matrix, die
auf einer fallenden oder steigenden Diagonalen liegen.  Die Produkte auf den fallenden Diagonalen
werden addiert, während die Produkte auf den steigenden Diagonalen subtrahiert werden.  

\begin{figure}[h]
  \centering
  \epsfig{file=Abbildungen/sarrus.eps, scale=0.1}
  \caption{Die Regel von Sarrus.}
  \label{fig:sarrus.eps}
\end{figure}
\noindent
\underline{\red{Beachten}}
Sie, dass die Regel von Sarrus nur für $3 \times 3$ Matrizen gilt, eine Verallgemeinerung dieser Regel für 
$4 \times 4$ Matrizen ist \red{falsch}!
\eoxs

\exercise
Leiten Sie die Formel von Sarrus aus der von Leibniz angegebenen Formel ab. 
\eox


\begin{figure}[!ht]
\centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  firstnumber   = 1,
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.0cm,
                  xrightmargin  = 0.0cm,
                ]
   inv := procedure(r) {
       n := #r;
       return { [k,l] : k in {1 .. n}, l in {k+1 .. n} | L[k] > L[l] };
   };
   signum := procedure(l) {
       return (-1) ** #inv(l);
   };
   allPermutations := procedure(S) {
       if (S == {}) {
           return { [] };
       }
       return { [k] + L : k in S, L in allPermutations(S - {k}) };
   };
   determinant := procedure(n) {
       det := "";
       Sn  := allPermutations({1 .. n});
       assert(#Sn == n!, "Wrong number of permutations!");
       for (L in Sn) {
           if (sgn(L) == 1) { sign := " + "; } else { sign := " - "; }
           product := join(["a[$i$][$L[i]$]" : i in [1 .. n]], " * ");
           det += " " * 6 + sign + product + "\n";
       }
       return det;
   };
\end{Verbatim} 
\vspace*{-0.3cm}
\caption{Berechnung der Leibniz-Formel in \textsc{SetlX}.}
\label{fig:determinant-formula.stlx}
\end{figure}

Abbildung \ref{fig:determinant-formula.stlx} auf Seite \pageref{fig:determinant-formula.stlx} zeigt ein Programm zur
Berechnung der oben angegebenen Leibniz-Formel zur Berechnung der Determinante.  Für $n=4$ erhalten
wir beispielsweise die in Abbildung \ref{fig:determinant.out} auf Seite
\pageref{fig:determinant.out} gezeigte Ausgabe.  Es ist offensichtlich, dass die Leibniz-Formel für
große Werte von $n$ in der Praxis unbrauchbar ist, denn die
Menge $\mathcal{S}_n$ aller Permutationen der Zahlen $\{1,\cdots,n\}$ enthält $n!$ verschiedene
Elemente und die Fakultätsfunktion wächst schneller als jede Potenz.  Genauer gilt die 
\href{https://de.wikipedia.org/wiki/Stirlingformel}{Stirling-Formel}
\\[0.2cm]
\hspace*{1.3cm}
$\ds n! \approx \sqrt{2 \cdot \pi \cdot n\;} \cdot \left(\frac{n}{e}\right)^n$.
\\[0.2cm]
Wir werden daher
ein anderes Verfahren zur Berechnung der Determinante entwickeln, bei dem nur etwa $n^3$
Rechenoperationen notwendig sind.  Zu diesem Zweck beweisen wir zunächst einige Eigenschaften der
Determinante.  

\begin{figure}[!ht]
\centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  firstnumber   = 1,
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.8cm,
                  xrightmargin  = 0.8cm,
                ]
    det(A) = a[1, 1] * a[2, 2] * a[3, 3] * a[4, 4]
           - a[1, 1] * a[2, 2] * a[3, 4] * a[4, 3]
           - a[1, 1] * a[2, 3] * a[3, 2] * a[4, 4]
           + a[1, 1] * a[2, 3] * a[3, 4] * a[4, 2]
           + a[1, 1] * a[2, 4] * a[3, 2] * a[4, 3]
           - a[1, 1] * a[2, 4] * a[3, 3] * a[4, 2]
           - a[1, 2] * a[2, 1] * a[3, 3] * a[4, 4]
           + a[1, 2] * a[2, 1] * a[3, 4] * a[4, 3]
           + a[1, 2] * a[2, 3] * a[3, 1] * a[4, 4]
           - a[1, 2] * a[2, 3] * a[3, 4] * a[4, 1]
           - a[1, 2] * a[2, 4] * a[3, 1] * a[4, 3]
           + a[1, 2] * a[2, 4] * a[3, 3] * a[4, 1]
           + a[1, 3] * a[2, 1] * a[3, 2] * a[4, 4]
           - a[1, 3] * a[2, 1] * a[3, 4] * a[4, 2]
           - a[1, 3] * a[2, 2] * a[3, 1] * a[4, 4]
           + a[1, 3] * a[2, 2] * a[3, 4] * a[4, 1]
           + a[1, 3] * a[2, 4] * a[3, 1] * a[4, 2]
           - a[1, 3] * a[2, 4] * a[3, 2] * a[4, 1]
           - a[1, 4] * a[2, 1] * a[3, 2] * a[4, 3]
           + a[1, 4] * a[2, 1] * a[3, 3] * a[4, 2]
           + a[1, 4] * a[2, 2] * a[3, 1] * a[4, 3]
           - a[1, 4] * a[2, 2] * a[3, 3] * a[4, 1]
           - a[1, 4] * a[2, 3] * a[3, 1] * a[4, 2]
           + a[1, 4] * a[2, 3] * a[3, 2] * a[4, 1]
\end{Verbatim}
\vspace*{-0.3cm}
\caption{Ausgabe der Funktion \texttt{determinant} aus Abbildung \ref{fig:determinant-formula.stlx} für $n=4$.}
\label{fig:determinant.out}
\end{figure}



\begin{Lemma} \label{lemma:sigmatau}
Es sei $\tau$ eine Transposition.  Dann gilt
\\[0.2cm]
\hspace*{1.3cm}
$\tau \circ \mathcal{S}_n := \{ \tau \circ \mu \mid \mu \in \mathcal{S}_n \} = \mathcal{S}_n$.
\end{Lemma}

\proof
Wir zerlegen den Beweis in zwei Teile:
\begin{enumerate}
\item $\{ \tau \circ \mu \mid \mu \in \mathcal{S}_n \} \subseteq \mathcal{S}_n$.

      Der Nachweis dieser Inklusion folgt aus der Tatsache, dass die Gruppe der Permutationen
      $\mathcal{S}_n$ unter dem relationalen Produkt abgeschlossen ist, was $\tau \circ \mu \in \mathcal{S}_n$ für alle $\mu \in \mathcal{S}_n$ impliziert.
\item $\mathcal{S}_n \subseteq \{ \tau \circ \mu \mid \mu \in \mathcal{S}_n \}$.

      Sei $\sigma \in \mathcal{S}_n$.  Es ist zu zeigen, dass es eine Permutation $\mu \in \mathcal{S}_n$ gibt,
      so dass sich $\sigma$ als das relationale Produkt $\tau \circ \mu$ schreiben lässt.  Wir definieren diese
      Permutation $\mu$ als 
      \\[0.2cm]
      \hspace*{1.3cm}
      $\mu := \tau \circ \sigma$.
      \\[0.2cm]
      Offenbar ist $\mu \in \mathcal{S}_n$ und es gilt
      \\[0.2cm]
      \hspace*{1.3cm}
      $\tau \circ \mu = \tau \circ \tau \circ \sigma = \mathtt{id}_n \circ \sigma = \sigma$,
      \\[0.2cm]
      denn da $\tau$ eine Transposition ist, gilt $\tau \circ \tau = \mathtt{id}_n$, wobei
      $\mathtt{id}_n$ die identische Permutation bezeichnet.  
      \qeds
\end{enumerate}

\noindent
Wir werden bei den folgenden Sätzen eine Matrix $A \in \mathbb{K}^{n \times n}$ 
immer in der Form 
\\[0.2cm]
\hspace*{1.3cm}
$A = \left(
  \begin{array}{ll}
    \vec{a}_1 \\ \vdots \\ \vec{a}_n
  \end{array}
  \right)
$
\\[0.2cm]
angeben.  Dabei bezeichnet $\vec{a}_i$ den $i$-ten Zeilenvektor der Matrix $A = \bigl(a_{i,j}\bigr)_{i = 1,\cdots n \atop j=1,\cdots n}$, 
es gilt also
\\[0.2cm]
\hspace*{1.3cm}
$\vec{a}_i = [a_{i,1}, \cdots, a_{i,n}]$.


\begin{Satz}
  Die Funktion $\texttt{det}(A)$ ist \emph{\color{blue}alternierend}: Vertauschen wir zwei Zeilen der Matrix
  $A$, so dreht sich das Vorzeichen der Determinante um, genauer gilt für $k,l \in \{1,\cdots,n\}$
  mit $k < l$:
  \\[0.2cm]
  \hspace*{1.3cm}
$\texttt{det}\left(
  \begin{array}{ll}
    \vec{a}_1 \\ \vdots \\ \vec{a}_l \\ \vdots \\ \vec{a}_k \\ \vdots \\ \vec{a}_n
  \end{array} 
  \right) = - \texttt{det}\left(
  \begin{array}{ll}
    \vec{a}_1 \\ \vdots \\ \vec{a_k} \\ \vdots \\ \vec{a}_l \\ \vdots \\ \vec{a}_n
  \end{array} 
  \right)
$.
\\[0.2cm]
Hier ist 
\\[0.2cm]
\hspace*{1.3cm}
$A = \left(
  \begin{array}{ll}
    \vec{a}_1 \\ \vdots \\ \vec{a}_k \\ \vdots \\ \vec{a}_l \\ \vdots \\ \vec{a}_n
  \end{array} 
  \right),
$
\quad während die Matrix \quad
$\left(
  \begin{array}{ll}
    \vec{a}_1 \\ \vdots \\ \vec{a}_l \\ \vdots \\ \vec{a}_k \\ \vdots \\ \vec{a}_n
  \end{array} 
  \right) =: \tau(A)
$
\\[0.2cm]
aus $A$ dadurch entsteht, dass wir die $k$-te Zeile von $A$ mit der $l$-ten Zeile $A$ vertauschen.
\end{Satz}

\proof
Es sei $\tau = \pair(k,l)$ die Transposition, die $k$ mit $l$ vertauscht.  Wir definieren $\tau(A)$
als die Matrix, die aus der Matrix $A$ durch Vertauschen der $k$-ten Zeile mit der $l$-ten Zeile
hervorgeht.  In Komponentenschreibweise haben wir dann
\\[0.2cm]
\hspace*{1.3cm}
$\tau(A) = \bigl(a_{\tau(i),j}\bigr)_{{i = 1,\cdots,n} \atop {j = 1,\cdots,n}}$.
\\[0.2cm]
Nach Definition der Determinante gilt
\\[0.2cm]
\hspace*{0.8cm}
$
\begin{array}[b]{lcl}
\texttt{det}\bigl(\tau(A)\bigr) & = & \ds
 \sum\limits_{\sigma \in \mathcal{S}_n} \texttt{sgn}(\sigma) \cdot a_{\tau(1),\sigma(1)} \cdot \mbox{$\dots$} \cdot a_{\tau(k),\sigma(k)} \cdot 
            \cdot  \mbox{$\dots$} \cdot a_{\tau(l),\sigma(l)} \cdot  \mbox{$\dots$} \cdot a_{\tau(n),\sigma(n)} \\[0.5cm]
& = & \ds
 \sum\limits_{\sigma \in \mathcal{S}_n} \texttt{sgn}(\sigma) \cdot a_{1,\sigma(1)} \cdot \mbox{$\dots$} \cdot a_{l,\sigma(k)} 
            \cdot \mbox{$\dots$} \cdot a_{k,\sigma(l)} \cdot \mbox{$\dots$} \cdot a_{n,\sigma(n)}, \\[0.4cm]
& & \mbox{denn $\tau(i) = i$ für alle $i \not\in \{k,l\}$, $\tau(k) = l$ und $\tau(l) = k$.} \\[0.2cm]
& = & \ds
 \sum\limits_{\sigma \in \mathcal{S}_n} \texttt{sgn}(\sigma) \cdot a_{1,\sigma(1)} \cdot \mbox{$\dots$} \cdot a_{k,\sigma(l)} 
            \cdot \mbox{$\dots$} \cdot a_{l,\sigma(k)} \cdot \mbox{$\dots$} \cdot a_{n,\sigma(n)}
            \\[0.4cm]
& & \mbox{Hier wird nur das Kommutativ-Gesetz benutzt.} \\[0.2cm]
& = & \ds
 \sum\limits_{\sigma \in \mathcal{S}_n} \texttt{sgn}(\sigma) \cdot a_{1,\sigma(\tau(1))} \cdot \mbox{$\dots$} \cdot a_{k,\sigma(\tau(k))} 
            \cdot \mbox{$\dots$} \cdot a_{l,\sigma(\tau(l))} \cdot \mbox{$\dots$} \cdot
            a_{n,\sigma(\tau(n))}, \\[0.4cm]
& & \mbox{denn $\tau(k) = l$, $\tau(l) = k$ und für $i \not\in \{k,l\}$ gilt $\tau(i) = i$.} \\[0.2cm]
& = & \ds
 \sum\limits_{\sigma \in \mathcal{S}_n} \texttt{sgn}(\sigma) \cdot a_{1,\tau\circ\sigma(1)} \cdot \mbox{$\dots$} \cdot a_{n,\tau\circ\sigma(n)} \\[0.4cm]
& = & \ds
 \sum\limits_{\mu \in \mathcal{S}_n} \texttt{sgn}(\tau\circ\mu) \cdot a_{1,\tau\circ(\tau\circ\mu)(1)} \cdot \mbox{$\dots$} \cdot 
            a_{n,\tau\circ(\tau\circ\mu)(n)} \\[0.4cm]
&& \mbox{In diesem Schritt haben wir alle $\sigma$ durch $\tau\circ\mu$ ersetzt,} \\
&& \mbox{was nach dem Lemma \ref{lemma:sigmatau} erlaubt ist.} \\[0.2cm]
& = & \ds
 \sum\limits_{\mu \in \mathcal{S}_n} \texttt{sgn}(\tau) \cdot \texttt{sgn}(\mu) \cdot a_{1,\tau\circ(\tau\circ\mu)(1)} \cdot \mbox{$\dots$} \cdot 
            a_{n,\tau\circ(\tau\circ\mu)(n)}, \\[0.4cm]
&& \mbox{denn nach Satz \ref{satz:signum} gilt $\texttt{sgn}(\tau \circ \mu) = \texttt{sgn}(\tau) \cdot \texttt{sgn}(\mu)$} \\[0.2cm]
& = & \ds
 - \sum\limits_{\mu \in \mathcal{S}_n} \texttt{sgn}(\mu) \cdot a_{1,(\tau\circ\tau)\circ\mu(1)} \cdot \mbox{$\dots$} \cdot 
            a_{n,(\tau\circ\tau)\circ\mu(n)}, \\[0.4cm]
&& \mbox{denn da $\tau$ eine Transposition ist, gilt $\texttt{sgn}(\tau) = -1$.} \\
&& \mbox{Außerdem haben wir benutzt, dass $\tau \circ(\tau\circ\mu) = (\tau \circ\tau)\circ\mu$ ist.} \\[0.2cm]
& = & \ds
 - \sum\limits_{\mu \in \mathcal{S}_n} \texttt{sgn}(\mu) \cdot a_{1,\mu(1)} \cdot \mbox{$\dots$} \cdot 
            a_{n,\mu(n)} \\[0.4cm]
&& \mbox{Da $\tau$ eine Transposition ist, gilt $\tau \circ \tau = \mathtt{id}_n$ und $\mathtt{id}_n \circ \mu = \mu$.} \\[0.2cm]
& = & -\texttt{det}(A).
 \end{array}
$
\qed

\remark
Der gerade bewiesene Satz ist der am schwersten zu beweisende Satz in der Theorie der
Determinanten.  Das liegt daran, dass dies der einzige Satz in dieser Theorie ist, bei dessen Beweis
wir die Eigenschaften der Vorzeichen-Funktion $\texttt{sgn}$ tatsächlich benötigen.  
\eoxs

\exercise
Wir definieren zu einer Matrix $A \in \mathbb{K}^{n \times n}$ der Form $A = (a_{i,j})$ die \emph{\color{blue}transponierte Matrix} 
$A^\mathrm{T} = (a^\mathrm{T}_{i,j})$ indem wir
\\[0.2cm]
\hspace*{1.3cm}
$a^\mathrm{T}_{i,j} := a_{j,i}$ \quad für alle $i,j \in \{1,\cdots,n\}$
\\[0.2cm]
setzen.  Die Zeilen der transponierten Matrix $A^\mathrm{T}$ sind also die Spalten der ursprünglichen Matrix $A$. 
Beispielsweise haben wir für
\\[0.2cm]
\hspace*{1.3cm}
$A = \left(
  \begin{array}[c]{lll}
    1 & 2 & 3     \\
    4 & 5 & 6     \\
    7 & 8 & 9            
  \end{array}\right)
$
\quad die transponierte Matrix \quad
$A^\mathrm{T} = \left(
  \begin{array}[c]{lll}
    1 & 4 & 7     \\
    2 & 5 & 8     \\
    3 & 6 & 9            
  \end{array}\right)
$.
\\[0.2cm]
Zeigen Sie, dass 
\\[0.2cm]
\hspace*{1.3cm}
$\texttt{det}(A^\mathrm{T}) = \texttt{det}(A)$
\\[0.2cm]
gilt.  Zum Nachweis dieses Satzes ist es nützlich, die beiden folgenden Lemmata nachzuweisen:
\begin{enumerate}
\item $\mathcal{S}_n^{-1} := \{ \sigma^{-1} \mid \sigma \in \mathcal{S}_n \} = \mathcal{S}_n$.
\item $\bigl\{ \langle j, \sigma^{-1}(j)\rangle \mid j \in \{1,\cdots,n\} \bigr\} = \bigl\{ \langle \sigma(i),i\rangle \mid i \in \{1,\cdots,n\} \bigr\}$
       \quad für alle $\sigma \in \mathcal{S}_n$.
       \eox
\end{enumerate}

\solution
Wir zeigen zunächst die beiden als Hinweis gegebenen Lemmata.
\begin{enumerate}
\item Wir zeigen  $\{ \sigma^{-1} \mid \sigma \in \mathcal{S}_n \} = \mathcal{S}_n$.

      Offenbar gilt $\{ \sigma^{-1} \mid \sigma \in \mathcal{S}_n \} \subseteq \mathcal{S}_n$, denn das Inverse einer Permutation ist wieder eine Permutation. 
      Um zu zeigen, dass auch die Inklusion
      \\[0.2cm]
      \hspace*{1.3cm}
      $\mathcal{S}_n \subseteq \{ \sigma^{-1} \mid \sigma \in \mathcal{S}_n \}$
      \\[0.2cm]
      wahr ist, nehmen wir an, dass $\mu$ eine beliebige Permutation aus $\mathcal{S}_n$ ist.  Wir wollen 
      \\[0.2cm]
      \hspace*{1.3cm}
      $\mu \in \{ \sigma^{-1} \mid \sigma \in \mathcal{S}_n \}$ 
      \\[0.2cm]
      nachweisen.  Daher müssen wir
      eine Permutation $\sigma$ finden, so dass
      \\[0.2cm]
      \hspace*{1.3cm}
      $\mu = \sigma^{-1}$
      \\[0.2cm]
      gilt, denn dann ist $\mu$ ein Element der Menge $\{ \sigma^{-1} \mid \sigma \in \mathcal{S}_n \}$.
      Wir definieren $\sigma := \mu^{-1}$.  Dann gilt
      \\[0.2cm]
      \hspace*{1.3cm}
      $\sigma^{-1} = \bigl(\mu^{-1}\bigr)^{-1} = \mu$
      \\[0.2cm]
      und damit ist  $\mu \in \{ \sigma^{-1} \mid \sigma \in \mathcal{S}_n \}$ gezeigt.  $\green{\surd}$
\item Wir zeigen, dass für alle $\sigma \in \mathcal{S}_n$ die Gleichung
      \\[0.2cm]
      \hspace*{1.3cm}
      $\bigl\{ \langle j, \sigma^{-1}(j)\rangle \mid j \in \{1,\cdots,n\} \bigr\} = \bigl\{ \langle \sigma(i),i\rangle \mid i \in \{1,\cdots,n\} \bigr\}$
      \\[0.2cm]
      gilt.  Wir zerlegen den Beweis dieser Mengen-Gleichung in zwei Teile.
      \begin{description}
      \item[``$\subseteq$'']:  Sei $j \in \{1, \cdots, n\}$ gegeben.  Um zu zeigen, dass
                     \\[0.2cm]
                     \hspace*{1.3cm}
                     $\langle j, \sigma^{-1}(j)\rangle \in \bigl\{ \langle \sigma(i), i\rangle \mid i \in \{1,\cdots,n\} \bigr\}$
                     \\[0.2cm]
                     gilt, müssen wir ein $i \in \{1, \cdots, n\}$ finden, so dass die Gleichung
                     \\[0.2cm]
                     \hspace*{1.3cm}
                     $\langle j, \sigma^{-1}(j)\rangle = \langle \sigma(i), i\rangle$
                     \\[0.2cm]
                     erfüllt ist.  Diese Gleichung ist aber zu den beiden Gleichungen
                     \\[0.2cm]
                     \hspace*{1.3cm}
                     $j = \sigma(i)$ \quad und \quad  $\sigma^{-1}(j) = i$ 
                     \\[0.2cm]
                     äquivalent und es ist klar, dass beide Gleichungen richtig sind, wenn wir $i := \sigma^{-1}(j)$
                     definieren. $\green{\surd}$
      \item[``$\supseteq$'']: Sei $i \in \{1, \cdots, n\}$ gegeben.  Um zu zeigen, dass
                     \\[0.2cm]
                     \hspace*{1.3cm}
                     $\langle \sigma(i), i\rangle \in \bigl\{ \langle j, \sigma^{-1}(j)\rangle \mid j \in \{1,\cdots,n\} \bigr\}$
                     \\[0.2cm]
                     gilt, müssen wir ein $j \in \{1, \cdots, n\}$ finden, so dass die Gleichung
                     \\[0.2cm]
                     \hspace*{1.3cm}
                     $\langle \sigma(i), i \rangle =\langle j, \sigma^{-1}(j) \rangle$
                     \\[0.2cm]
                     erfüllt ist.  Diese Gleichung ist aber zu den beiden Gleichungen
                     \\[0.2cm]
                     \hspace*{1.3cm}
                     $\sigma(i) = j$ \quad und \quad $i = \sigma^{-1}(j)$
                     \\[0.2cm]
                     äquivalent und es ist klar, dass beide Gleichungen gelten, wenn wir $j := \sigma(i)$
                     definieren. $\green{\surd}$
      \end{description}
\end{enumerate}
Nach diesen Vorüberlegungen führen wir jetzt den eigentlichen Beweis der Behauptung
\\[0.2cm]
\hspace*{1.3cm}
$\texttt{det}\bigl(A^\mathrm{T}\bigr) = \texttt{det}(A)$.
\\[0.2cm]
Wir berechnen dazu $\texttt{det}\bigl(A^\mathrm{T}\bigr)$ nach der Formel von Leibniz:
\\[0.2cm]
\hspace*{1.3cm}
$
\begin{array}[t]{lcl}
      \texttt{det}\bigl(A^\mathrm{T}\bigr) 
& = & \ds\sum\limits_{\sigma\in\mathcal{S}_n} \texttt{sgn}(\sigma) \cdot \prod\limits_{i=1}^n a_{i,\sigma(i)}^\mathrm{T} \\[0.4cm] 
& = & \ds\sum\limits_{\sigma\in\mathcal{S}_n} \texttt{sgn}(\sigma) \cdot \prod\limits_{i=1}^n a_{\sigma(i),i} \\[0.4cm] 
&   & \mbox{wegen $a_{i,j}^\mathrm{T} = a_{j,i}$} \\[0.2cm] 
& = & \ds\sum\limits_{\sigma\in\mathcal{S}_n} \texttt{sgn}(\sigma) \cdot \prod\limits_{j=1}^n a_{j,\sigma^{-1}(j)} \\[0.4cm] 
&   & \mbox{wegen $\bigl\{ \langle j, \sigma^{-1}(j)\rangle \mid j \in \{1,\cdots,n\} \bigr\} = \bigl\{ \langle \sigma(i),i\rangle \mid i \in \{1,\cdots,n\} \bigr\}$} 
      \\[0.2cm] 
& = & \ds\sum\limits_{\sigma\in\mathcal{S}_n} \texttt{sgn}\bigl(\sigma^{-1}\bigr) \cdot \prod\limits_{j=1}^n a_{j,\sigma(j)} \\[0.4cm] 
&   & \mbox{wegen $\{ \sigma^{-1} \mid \sigma \in \mathcal{S}_n \} = \mathcal{S}_n$} \\[0.2cm] 
& = & \ds\sum\limits_{\sigma\in\mathcal{S}_n} \texttt{sgn}(\sigma) \cdot \prod\limits_{j=1}^n a_{j,\sigma(j)} \\[0.4cm] 
&   & \mbox{wegen $\texttt{sgn}\bigl(\sigma^{-1}\bigr) = \texttt{sgn}(\sigma)$} \\[0.2cm] 
& = & \texttt{det}(A)
\end{array}
$
\\[0.2cm]
Damit ist der Beweis abgeschlossen.  \qed

\exercise
Zeigen Sie, dass für die $n \times n$ Einheits-Matrix $\mathrm{E}_n = (\delta_{i,j})_{{i =1,\cdots n} \atop {j=1,\cdots,n}}$
\\[0.2cm]
\hspace*{1.3cm}
$\texttt{det}(\mathrm{E}_n) = 1$
\\[0.2cm]
gilt. 
\eoxs

\exercise 
Zeigen Sie, dass die Funktion $\texttt{det}$ bezüglich jeder Zeile der Matrix \emph{\color{blue}additiv} ist: Zeigen Sie
also, dass in dem Fall, dass die Matrix $A$ die Form
\\[0.2cm]
\hspace*{1.3cm}
$\texttt{det}\left(
  \begin{array}{ll}
    \vec{a}_1 \\ \vdots \\ \vec{a}_k \\ \vdots \\ \vec{a}_n
  \end{array} \right)
$
\\[0.2cm]
hat und sich die $k$-te Zeile als 
\\[0.2cm]
\hspace*{1.3cm}
$\vec{a}_k = \vec{b} + \vec{c}$
\\[0.2cm]
schreiben lässt, das Folgende gilt:
  \\[0.2cm]
  \hspace*{1.3cm}
$\texttt{det}\left(
  \begin{array}{c}
    \vec{a}_1 \\ \vdots \\ \vec{a}_k \\ \vdots \\ \vec{a}_n
  \end{array} 
  \right) =
\texttt{det}\left(
  \begin{array}{c}
    \vec{a}_1 \\ \vdots \\ \vec{b} + \vec{c}\\ \vdots \\ \vec{a}_n
  \end{array} 
  \right)
 = \texttt{det}\left(
  \begin{array}{c}
    \vec{a}_1 \\ \vdots \\ \vec{b} \\ \vdots \\ \vec{a}_n
  \end{array}  \right)
+
   \texttt{det}\left(
  \begin{array}{c}
    \vec{a}_1 \\ \vdots \\ \vec{c} \\ \vdots \\ \vec{a}_n
  \end{array} 
  \right)
$.
\eoxs

\exercise 
Zeigen Sie, dass die Funktion $\texttt{det}$ bezüglich jeder Zeile der Matrix \emph{\color{blue}homogen} ist: Zeigen Sie
also, dass in dem Fall, dass die Matrix $A$ die Form
\\[0.2cm]
\hspace*{1.3cm}
$\texttt{det}\left(
  \begin{array}{ll}
    \vec{a}_1 \\ \vdots \\ \vec{a}_k \\ \vdots \\ \vec{a}_n
  \end{array} \right)
$
\\[0.2cm]
hat und sich die $k$-te Zeile als 
\\[0.2cm]
\hspace*{1.3cm}
$\vec{a}_k = \lambda \cdot \vec{b}$
\\[0.2cm]
schreiben lässt, das Folgende gilt:
  \\[0.2cm]
  \hspace*{1.3cm}
$\texttt{det}\left(
  \begin{array}{c}
    \vec{a}_1 \\ \vdots \\ \vec{a}_k \\ \vdots \\ \vec{a}_n
  \end{array} 
  \right) =
\texttt{det}\left(
  \begin{array}{c}
    \vec{a}_1 \\ \vdots \\ \lambda \cdot \vec{b} \\ \vdots \\ \vec{a}_n
  \end{array} 
  \right)
 = \lambda \cdot \texttt{det}\left(
  \begin{array}{c}
    \vec{a}_1 \\ \vdots \\ \vec{b} \\ \vdots \\ \vec{a}_n
  \end{array}  \right)
$.
\eoxs


\remark
Die letzten Lemmata und die dazugehörigen Aufgaben können wir so zusammenfassen, dass wir sagen, dass
die Determinante eine \blue{normierte}, \blue{alternierende} \blue{Multilinear-Form} ist:
\begin{enumerate}
\item Normiertheit bedeutet
      \\[0.2cm]
      \hspace*{1.3cm}
      $\texttt{det}(\mathrm{E}_n) = 1$.
\item Alternierend ist die Determinante, weil
      \\[0.2cm]
      \hspace*{1.3cm}
      $\texttt{det}(\cdots, \vec{a}_ k, \cdots, \vec{a}_l, \cdots) = -\texttt{det}(\cdots, \vec{a}_l, \cdots, \vec{a}_k, \cdots)$
      \\[0.2cm]
      gilt.  Das Vertauschen zweier Spalten ändert das Vorzeichen der Determinante.  

      \textbf{Bemerkung:} 
      Eine analoge Gleichung gilt, wenn wir zwei Zeilen vertauschen.  Dies liegt einfach daran,
      dass $\texttt{det}(A^\mathrm{T}) = \texttt{det}(A)$ ist.
\item Multilinear ist die Determinante, weil
      \\[0.2cm]
      \hspace*{1.3cm}
      $\texttt{det}(\cdots, \alpha \cdot \vec{a} + \beta \cdot \vec{b}, \cdots) = 
         \alpha \cdot \texttt{det}(\cdots, \vec{a}, \cdots) +
         \beta  \cdot \texttt{det}(\cdots, \vec{b}, \cdots)
      $
      \\[0.2cm]
      gilt.  Die Determinanten-Funktion ist also in jeder Spalte sowohl additiv als auch homogen.

      \textbf{Bemerkung:} 
      Eine analoge Gleichung gilt, wenn wir statt zweier Spalten zwei Zeilen addieren.
      Dies liegt wieder daran, dass $\texttt{det}(A^\mathrm{T}) = \texttt{det}(A)$ ist.
\end{enumerate}
\href{http://en.wikipedia.org/wiki/Karl_Weierstrass}{Karl Theodor Wilhelm Weierstrass} (1815-1897)
hat gezeigt, dass die obigen Eigenschaften die Determinante bereits vollständig bestimmen: Jede Funktion
\\[0.2cm]
\hspace*{1.3cm}
$f: \mathbb{K}^{n \times n} \rightarrow \mathbb{K}$,
\\[0.2cm]
die normiert, alternierend und multilinear ist, stimmt mit der Determinanten-Funktion überein.  
Daher werden wir auch bei den folgenden Rechnungen nicht mehr auf die recht schwerfällige Definition
der Determinante nach Leibniz zurückgreifen, sondern wir werden nur noch die oben genannten
Eigenschaften benutzen.
\eoxs

\begin{Lemma}
  Es sei $A \in \mathbb{K}^{n \times n}$, es gelte $\mathbb{K} = \mathbb{R}$ oder $\mathbb{K}= \mathbb{C}$,  weiter sei  $k,l \in \{1,\cdots,n\}$ mit $k\not=l$ und die $k$-te
  Zeile von $A$ sei mit der $l$-ten Zeile von $A$ identisch.  Dann gilt
  \\[0.2cm]
  \hspace*{1.3cm}
  $\texttt{det}(A) = 0$.
\end{Lemma}

\proof
Wir definieren die Transposition $\tau := \pair(k,l)$.  Da die $k$-te Zeile mit der $l$-ten Zeile
identisch ist, gilt  $\tau(A) = A$.  Andererseits wissen wir, dass sich bei dem Vertauschen zweier
Zeilen das Vorzeichen der Determinante umdreht.  Also haben wir
\\[0.2cm]
\hspace*{1.3cm}
$\texttt{det}(A) = \texttt{det}(\tau(A)) = -\texttt{det}(A)$.
\\[0.2cm]
Daraus folgt 
\\[0.2cm]
\hspace*{1.3cm}
$2 \cdot \texttt{det}(A) = 0$
\\[0.2cm]
und da wir annehmen\footnote{
  Es gibt endliche Körper. in denen $1 + 1 = 0$ ist.  In einem solchen Fall dürften wir die obige Gleichung
  nicht durch 2 dividieren, denn dort gilt $2 = 0$. 
}, dass der Körper $\mathbb{K}$ entweder der Körper $\mathbb{R}$ der reellen
Zahlen oder der Körper $\mathbb{C}$ der komplexen Zahlen ist, folgt $\texttt{det}(A) = 0$.
\qed


\begin{Lemma} \label{lemma:det-add-line}
  Es sei $A \in \mathbb{K}^{n \times n}$, $k,l \in \{1,\cdots,n\}$ mit $k\not=l$, $\lambda \in \mathbb{K}$ und die Matrix $B$
  entstehe aus $A$, indem das $\lambda$-fache der $k$-ten Zeile zur $l$-ten Zeile addiert wird.
  Falls $A$ die Form
  \\[0.2cm]
  \hspace*{1.3cm}
  $A = \left(
  \begin{array}{c}
    \vec{a}_1 \\ \vdots \\ \vec{a}_k \\ \vdots \\ \vec{a}_l \\ \vdots \\ \vec{a}_n
  \end{array} \right)
  $ \quad
  hat, gilt also \quad
  $B = 
  \left(\begin{array}{c}
    \vec{a}_1 \\ \vdots \\ \vec{a}_k \\ \vdots \\ \vec{a}_l + \lambda \cdot \vec{a}_k \\ \vdots \\ \vec{a}_n
  \end{array} 
  \right)
$.
  \\[0.2cm]
  Dann gilt 
  \\[0.2cm]
  \hspace*{1.3cm}
  $\texttt{det}(B) = \texttt{det}(A)$.
\end{Lemma}

\proof
Zunächst betrachten wir die Matrix
\\[0.2cm]
\hspace*{1.3cm}
  $C := \left(
  \begin{array}{c}
    \vdots \\ \vec{a}_k \\ \vdots \\ \vec{a}_k \\ \vdots 
  \end{array} \right)
  $,
\\[0.2cm]
die aus $A$ dadurch hervorgeht, dass wir die $l$-te Zeile durch die $k$-te Zeile ersetzen.  Da in
$C$ dann die $k$-te Zeile mit der $l$-ten Zeile identisch ist, gilt nach dem gerade bewiesenen Lemma
\\[0.2cm]
\hspace*{1.3cm}
$\texttt{det}(C) = 0$. 
\\[0.2cm]
Definieren wir die Matrix $D$ dadurch, dass wir in $C$ die $l$-te Zeile mit $\lambda$
multiplizieren, so haben wir wegen der Homogenität der Funktion $\texttt{det}$
\\[0.2cm]
\hspace*{1.3cm}
$\texttt{det}(D) = \texttt{det}\left(
  \begin{array}{c}
    \vdots \\ \vec{a}_k \\ \vdots \\ \lambda \cdot \vec{a}_k \\ \vdots 
  \end{array} \right) =
  \lambda \cdot \texttt{det}\left(
  \begin{array}{c}
    \vdots \\ \vec{a}_k \\ \vdots \\ \vec{a}_k \\ \vdots 
  \end{array} \right)
= \lambda \cdot \texttt{det}(C) = \lambda \cdot 0 = 0
$.
\\[0.2cm]
Aufgrund der Additivität der Funktion $\texttt{det}$ haben wir insgesamt
\\[0.2cm]
\hspace*{1.3cm}
$
\begin{array}[b]{lcl}
\texttt{det}(B) & = & 
  \texttt{det}\left(\begin{array}{c}
    \vdots \\ \vec{a}_k \\ \vdots \\ \vec{a}_l + \lambda \cdot \vec{a}_k \\ \vdots 
  \end{array} 
  \right) \\[1.5cm]
 & = &
\texttt{det}\left(\begin{array}{c}
  \vdots \\ \vec{a}_k \\ \vdots \\ \vec{a}_l \\ \vdots 
  \end{array} 
  \right) +
  \texttt{det}\left(\begin{array}{c}
    \vdots \\ \vec{a}_k \\ \vdots \\ \lambda \cdot \vec{a}_k \\ \vdots 
  \end{array} 
  \right) \\[1.5cm]
 & = & \texttt{det}(A) + \texttt{det}(D) \\[0.2cm]
 & = & \texttt{det}(A) + 0 \\[0.2cm] 
 & = & \texttt{det}(A). 
\end{array}$ \qed

\remark
Ersetzen wir in dem letzten Lemma den Begriff ``\emph{\color{blue}Zeile}'' durch ``\emph{\color{blue}Spalte}'', 
so bleibt das Lemma gültig, denn die Determinante der transponierten Matrix $A^\mathrm{T}$ ist gleich der
Determinante von $A$ und beim Übergang von $A$ zur transponierten Matrix $A^\mathrm{T}$ werden aus den
Spalten der Matrix $A$ die Zeilen der Matrix $A^\mathrm{T}$. 
\eox

\begin{Lemma}
  Ist $A = (\vec{a}_1, \cdots, \vec{a}_n) \in \mathbb{K}^{n \times n}$ und ist die Menge der
  Spalten der Matrix $A$, also die Menge $\{ \vec{a}_1, \cdots, \vec{a}_n \}$, linear abhängig,
  so gilt
  \\[0.2cm]
  \hspace*{1.3cm}
  $\texttt{det}(A) = 0$.
\end{Lemma}

\proof
Zunächst bemerken wir, dass die Determinante eine Matrix $B$ sicher dann den Wert $0$ hat, wenn eine
der Spalten von $B$ der Null-Vektor $\vec{0}$ ist.  Dies können wir wie folgt aus der Homogenität
der Determinanten-Funktion schließen: Nehmen wir an, dass die $i$-te Spalte von $B$ der Null-Vektor
ist, es gelte also
\\[0.2cm]
\hspace*{1.3cm}
$B = (\vec{b}_1,\cdots, \vec{b}_{i-1}, \vec{0}, \vec{b}_{i+1}, \cdots, \vec{b}_n)$.
\\[0.2cm]
Dann haben wir 
\\[0.2cm]
\hspace*{1.3cm}
$
\begin{array}[t]{lcl}
 \texttt{det}(B) & = & \texttt{det}(\vec{b}_1,\cdots, \vec{b}_{i-1}, \vec{0}, \vec{b}_{i+1}, \cdots, \vec{b}_n) \\[0.2cm] 
                 & = & \texttt{det}(\vec{b}_1,\cdots, \vec{b}_{i-1}, 0 \cdot \vec{0}, \vec{b}_{i+1}, \cdots, \vec{b}_n) \\[0.2cm]  
                 & = & 0 \cdot \texttt{det}(\vec{b}_1,\cdots, \vec{b}_{i-1}, \vec{0}, \vec{b}_{i+1}, \cdots, \vec{b}_n) \\[0.2cm]  
                 & = & 0.
\end{array}
$
\\[0.2cm]
Aus der Voraussetzung, dass die Menge $\{\vec{a}_1, \cdots, \vec{a}_n\}$ linear abhängig ist, folgt, dass es Skalare \\
 $\lambda_1,\cdots, \lambda_n \in \mathbb{K}$ gibt, so dass 
\\[0.2cm]
\hspace*{1.3cm}
$\lambda_1 \cdot \vec{a}_1 + \cdots + \lambda_n \cdot \vec{a}_n = \vec{0}$
\\[0.2cm]
ist, wobei wenigstens für ein $k \in \{1,\cdots,n\}$ die Ungleichung $\lambda_k \not= 0$ gilt.
Teilen wir die obige Gleichung dann durch dieses $\lambda_k$ und bringen die Ausdrücke mit $i\not=k$
auf die andere Seite der Gleichung, so lässt sich der Vektor $\vec{a}_k$ wie folgt als Linear-Kombination 
der übrigen Vektoren $\vec{a}_i$ mit $i \not= k$ schreiben:
\\[0.2cm]
\hspace*{1.3cm}
$\ds\vec{a}_k = - \sum\limits_{i=1 \atop i \not= k}^n \bruch{\lambda_i}{\lambda_k} \cdot \vec{a}_i$.
\\[0.2cm]
Durch wiederholtes Anwenden des letzten Satzes können wir zur $k$-ten Spalte der Matrix $A$
nacheinander das $-\bruch{\lambda_i}{\lambda_k}$-fache der $i$-ten Spalte für alle $i \in \{1,\cdots,n\} \backslash \{k\}$ 
hinzuaddieren, ohne dass sich dabei der Wert der Determinante der Matrix ändert:
\\[0.2cm]
\hspace*{1.3cm}
$
\begin{array}[t]{lcl}
  \texttt{det}(A) & = & \texttt{det}(\vec{a}_1, \cdots, \vec{a}_k, \cdots, \vec{a}_n) \\[0.2cm]
                  & = & \texttt{det}(\vec{a}_1, \cdots, 
                        \vec{a}_k - \bruch{\lambda_1}{\lambda_k} \cdot \vec{a}_1, \cdots, \vec{a}_n) \\[0.2cm]
                  & = & \texttt{det}(\vec{a}_1, \cdots, 
                        \vec{a}_k -\bruch{\lambda_1}{\lambda_k} \cdot \vec{a}_1 - \bruch{\lambda_2}{\lambda_k} \cdot \vec{a}_2,
                        \cdots, \vec{a}_n) \\[0.2cm] 
                  & = & \vdots \\
                  & = & \texttt{det}(\vec{a}_1, \cdots, 
                        \vec{a}_k - \sum\limits_{i=1 \atop i \not= k}^n \bruch{\lambda_i}{\lambda_k} \cdot \vec{a}_i,
                        \cdots, \vec{a}_n) \\[0.2cm] 
                  & = & \texttt{det}(\vec{a}_1, \cdots, 
                        \vec{0},
                        \cdots, \vec{a}_n) \\[0.2cm] 
                 & = & 0.
\end{array}
$
\\[0.2cm]
Im letzten Schritt haben wir dann eine Matrix, deren $k$-te Spalte den Wert $\vec{0}$ hat und
nach der am Beginn dieses Beweises gemachten Bemerkung ist die Determinante dieser Matrix $0$.
\qed


Genau wie wir für eine lineare Abbildung $f\in \mathcal{L}(V,W)$ im letzten Kapitel die Mengen
$\mathtt{Kern}(f)$ und $\mathtt{Bild}(f)$ definiert haben, können wir auch für eine Matrix $A \in \mathbb{K}^{n \times m}$, die
Mengen $\mathtt{\color{blue}Kern}(A)$ und $\mathtt{\color{blue}Bild}(A)$ definieren:
\begin{enumerate}
\item $\mathtt{Kern}(A) := \bigl\{ \vec{x} \in \mathbb{K}^m \bigm| A \cdot \vec{x} = \vec{0} \bigr\}$, 
\item $\mathtt{Bild}(A) := \bigl\{ A \cdot \vec{x} \bigm| \vec{x} \in  \mathbb{K}^m \bigr\}$.
\end{enumerate}
Da die Abbildung $\vec{x} \mapsto A \cdot \vec{x}$ eine lineare Abbildung ist, ist $\mathtt{Kern}(A)$ ein Unter-Vektorraum 
von $\mathbb{K}^m$ und $\mathtt{Bild}(A)$ ist ein Unter-Vektorraum von $\mathbb{K}^n$.  Nach dem Dimensions-Satz folgt dann
\\[0.2cm]
\hspace*{1.3cm}
$m = \mathtt{dim}(\mathtt{Kern}(A)) + \mathtt{dim}(\mathtt{Bild}(A))$.
\\[0.2cm]
Im Falle eine quadratischen Matrix gilt $m = n$ und folglich gilt in diesem Fall
\\[0.2cm]
\hspace*{1.3cm}
$\mathtt{dim}(\mathtt{Kern}(A)) = 0 \;\Leftrightarrow\; n = \mathtt{dim}(\mathtt{Bild}(A))$. 
\\[0.2cm]
Aus dieser \"{A}quivalenz ergibt sich der folgende Satz:

\begin{Satz}
Ist $A \in \mathbb{K}^{n \times n}$ eine quadratische Matrix, dann gilt:
\\[0.2cm]
\hspace*{1.3cm}
$A$ ist genau dann invertierbar, wenn $\mathtt{Kern}(A) = \bigl\{ \vec{0} \bigr\}$ gilt.
\end{Satz}

\proof
Wir definieren eine lineare Abbildung
\\[0.2cm]
\hspace*{1.3cm}
$f:\mathbb{K}^n \rightarrow \mathbb{K}^n$ \quad durch \quad $f(\vec{x}) := A \cdot \vec{x}$.
\\[0.2cm]
Dann haben wir die folgende Kette von Äquivalenzen:
\\[0.2cm]
\hspace*{1.3cm}
$
\begin{array}[t]{cl}
                & \mbox{$f$ ist injektiv}                       \\[0.2cm]
\Leftrightarrow & \mathtt{Kern}(f) = \bigl\{ \vec{0} \bigr\}    \\[0.2cm]
\Leftrightarrow & \mathtt{Kern}(A) = \bigl\{ \vec{0} \bigr\}    \\[0.2cm]
\Leftrightarrow & \mathtt{dim}\bigl(\mathtt{Kern}(A)\bigr) = 0  \\[0.2cm]
\Leftrightarrow & \mathtt{dim}\bigl(\mathtt{Bild}(A)\bigr) = n  \\[0.2cm]
\Leftrightarrow & \mathtt{Bild}(A) = \mathbb{K}^n         \\[0.2cm]
\Leftrightarrow & \mathtt{Bild}(f) = \mathbb{K}^n          \\[0.2cm]
\Leftrightarrow & \mbox{$f$ ist surjektiv}        
\end{array}
$
\\[0.2cm]
Folglich ist $f$ genau dann injektiv wenn es auch surjektiv ist.  Wir spalten den Rest des Beweises in zwei Teile auf.
\begin{enumerate}
\item Es gelte $\mathtt{Kern}(A) = \bigl\{\vec{0}\bigr\}$.  

      Nach dem oben Gesagten folgt dann, dass die Abbildung $f$ bijektiv ist und damit eine Umkehr-Abbildung besitzt.
      Bezeichnen wir die Umkehr-Abbildung mit $f^{-1}$, so können wir sehen, dass $f^{-1}$ ebenfalls
      eine lineare Abbildung ist, denn es gilt
      \\[0.2cm]
      \hspace*{1.3cm}
      $
\begin{array}[t]{lll}
                & f^{-1}(\vec{x} + \vec{y}) = f^{-1}(\vec{x}) + f^{-1}(\vec{y}) \\[0.2cm]
\Leftrightarrow & f\bigl(f^{-1}(\vec{x} + \vec{y})\bigr) = f\bigl(f^{-1}(\vec{x}) + f^{-1}(\vec{y})\bigr) & \mbox{denn $f$ ist injektiv} \\[0.2cm] 
\Leftrightarrow & \vec{x} + \vec{y} = f\bigl(f^{-1}(\vec{x})) + f\bigl(f^{-1}(\vec{y})\bigr)  & \mbox{denn $f$ ist linear} \\[0.2cm] 
\Leftrightarrow & \vec{x} + \vec{y} = \vec{x} + \vec{y}.
\end{array}
$
      \\[0.2cm]
      Folglich lässt sich die Umkehr-Abbildung $f^{-1}$ wieder durch eine Matrix $B$ darstellen, so dass
      sich die beiden Gleichungen
      \\[0.2cm]
      \hspace*{1.3cm}
      $f^{-1}\bigl(f(\vec{x})\bigr) = \vec{x}$ \quad und \quad
      $f\bigl(f^{-1}(\vec{x})\bigr) = \vec{x}$ 
      \\[0.2cm]
      dann in der Form 
      \\[0.2cm]
      \hspace*{1.3cm}
      $B \cdot A \cdot \vec{x} = \vec{x}$ \quad und \quad
      $A \cdot B \cdot \vec{x} = \vec{x}$ 
      \\[0.2cm]
      schreiben lassen.  Weil diese Gleichungen für alle $\vec{x} \in \mathbb{K}^n$ gelten, folgt
      \\[0.2cm]
      \hspace*{1.3cm}
      $B \cdot A = \mathrm{E}_n$ \quad und \quad $A \cdot B = \mathrm{E}_n$,
      \\[0.2cm]
      wobei $\mathrm{E}_n$ die $n \times n$ Einheits-Matrix bezeichnet.  Damit ist $B$ die zu $A$ inverse
      Matrix, es gilt also $B = A^{-1}$.  Folglich ist $A$ invertierbar.
\item Nun sei $A$ invertierbar.  Dann gibt es eine Matrix $B$, so dass
      \\[0.2cm]
      \hspace*{1.3cm}
      $A \cdot B = E_n = B \cdot A$ 
      \\[0.2cm]
      gilt.  Wir zeigen, dass $\mathtt{Kern}(A) = \bigl\{\vec{0}\bigr\}$ gilt.  Sei also
      \\[0.2cm]
      \hspace*{1.3cm}
      $\vec{x} \in \mathtt{Kern}(A)$
      \\[0.2cm]
      Nach Definition von $\mathtt{Kern}(A)$ heißt das
      \\[0.2cm]
      \hspace*{1.3cm}
      $A \cdot \vec{x} = \vec{0}$
      \\[0.2cm]
      Andererseits ist klar, dass
      \\[0.2cm]
      \hspace*{1.3cm}
      $A \cdot \vec{0} = \vec{0}$
      \\[0.2cm]
      gilt.  Damit haben wir
      \\[0.2cm]
      \hspace*{1.3cm}
      $A \cdot \vec{x} = A \cdot \vec{0}$
      \\[0.2cm]
      Multiplizieren wir diese Gleichung von links mit der zu $A$ inversen Matrix $B$, so erhalten wir die Gleichung
      \\[0.2cm]
      \hspace*{1.3cm}
      $B \cdot A \cdot \vec{x} = B \cdot A \cdot \vec{0}$,
      \\[0.2cm]
      die sich wegen $B \cdot A = E_n$ zu
      \\[0.2cm]
      \hspace*{1.3cm}
      $E_n \cdot \vec{x} = E_n \cdot \vec{0}$,
      \\[0.2cm]
      vereinfacht, woraus $\vec{x} = \vec{0}$ folgt.
      \qed
\end{enumerate}

\begin{Lemma} \label{lemma:ze}
Es sei $B \in \mathbb{K}^{n \times n}$ und es $\mathrm{ZE}$ sei eine Elementar-Matrix.
Weiter sei $\texttt{det}(B) \not= 0$.  Dann gilt auch 
\\[0.2cm]
\hspace*{1.3cm}
$\texttt{det}(\mathrm{ZE} \cdot B) \not= 0$.  
\end{Lemma}

\proof
Wir erbringen den Beweis der Behauptung durch eine Fallunterscheidung nach der Form der Elementar-Matrix $\mathrm{ZE}$.
\begin{enumerate}
\item Fall: $\mathrm{ZE} = \mathrm{ZA}_n(k,l,\alpha)$. 
  
      Multiplizieren wir die Matrix $B$ mit der Zeilen-Additions-Matrix $\mathrm{ZA}_n(k,l,\alpha)$,
      so besteht die Wirkung dieser Multiplikation darin, dass die $l$-te Zeile von $B$  mit $\alpha$ multipliziert
      zur $k$-ten Zeile von $B$ hinzu addiert wird.  Nach Lemma \ref{lemma:det-add-line}
      ändert sich der Wert der Determinante dabei nicht und folglich gilt
      \\[0.2cm]
      \hspace*{1.3cm}
      $\texttt{det}\bigl(\mathrm{ZA}_n(k,l,\alpha) \cdot B\bigr) = \texttt{det}(B) \not= 0$,
      \\[0.2cm]
      denn wir haben vorausgesetzt, dass $\texttt{det}(B) \not= 0$ gilt.
\item Fall: $\mathrm{ZE} = \mathrm{ZP}_n(k,l)$.

      Multiplizieren wir eine $n \times n$ Matrix $B$ von links mit
      $\mathrm{ZP}_n(k,l)$, so besteht der Effekt auf $B$ darin, dass die $k$-te Zeile von $B$ mit der
      $l$-ten Zeile von $B$ vertauscht wird.
      Da das Vertauschen zweier Zeilen das Vorzeichen der Determinante umdreht, haben wir
      \\[0.2cm]
      \hspace*{1.3cm}
      $\texttt{det}\bigl(\mathrm{ZP}_n(k,l) \cdot B\bigr) = -\texttt{det}(B) \not= 0$,
      \\[0.2cm]
      denn wir haben vorausgesetzt, dass $\texttt{det}(B) \not= 0$ gilt.
\item Fall: $\mathrm{ZE} = \mathrm{ZM}_n(k, \alpha)$ \quad mit $\alpha \not= 0$.

      Multiplizieren wir eine $n \times n$ Matrix $B$ von links mit
      $\textrm{ZM}_n(k,\alpha)$, so besteht der Effekt auf $B$ darin, dass die $k$-te Zeile von $B$
      mit $\alpha$ multipliziert wird.
      Auf Grund der Homogenität der Determinanten-Funktion folgt
      \\[0.2cm]
      \hspace*{1.3cm}
      $\texttt{det}\bigl(\mathrm{ZM}_n(k,\alpha) \cdot B) = \alpha \cdot \texttt{det}(B) \not= 0$,
      \\[0.2cm]
      denn einerseits ist $\alpha \not= 0$ und andererseits       
      haben wir vorausgesetzt, dass $\texttt{det}(B) \not= 0$ gilt. \qed
\end{enumerate}


\begin{Satz}
  Die Matrix $A \in \mathbb{K}^{n \times n}$ sei invertierbar.  Dann gilt $\texttt{det}(A) \not= 0$.
\end{Satz}

\proof
Im letzten Kapitel hatten wir gezeigt, wie sich zu einer invertierbaren Matrix $A$ die Inverse
Matrix $A^{-1}$ berechnen lässt.   Wir hatten damals die Matrix $A$ solange mit Elementar-Matrizen
multipliziert, bis wir $A$ zur Einheits-Matrix reduziert hatten.  Konkret hatten wie eine endliche
Folge $\mathrm{ZE}_1, \cdots, \mathrm{ZE}_k$ von Elementar-Matrizen berechnet, so dass die Gleichung
\\[0.2cm]
\hspace*{1.3cm}
$\mathrm{ZE}_k \cdot \mbox{\dots} \cdot \mathrm{ZE}_1 \cdot A = \mathrm{E}_n$
\\[0.2cm]
erfüllt war.  Wir wissen, dass die Elementar-Matrizen alle invertierbar sind.  Daher
lässt sich die obige Gleichung zu
\\[0.2cm]
\hspace*{1.3cm}
$A = \mathrm{ZE}_1^{-1} \cdot \mbox{\dots} \cdot \mathrm{ZE}_k^{-1} \cdot \mathrm{E}_n$
\\[0.2cm]
umschreiben.  Zusätzlich wissen wir, dass das Inverse einer Elementar-Matrix wieder eine Elementar-Matrix ist. 
Da $\mathrm{det}(E_n) = 1 \not= 0$ ist, können wir dann durch $k$-malige Anwendung von Lemma
\ref{lemma:ze} folgern, dass auch  
\\[0.2cm]
\hspace*{1.3cm}
$\mathrm{det}(\mathrm{ZE}_1^{-1} \cdot \mbox{\dots} \cdot \mathrm{ZE}_k^{-1} \cdot \mathrm{E}_n) \not = 0$
\\[0.2cm]
gilt und damit ist $\mathrm{det}(A) \not= 0$ gezeigt. \qed


\exercise
Zeigen Sie, dass für eine invertierbare Matrix $A \in \mathbb{K}^{n \times n}$ und eine beliebige
Matrix $B \in \mathbb{K}^{n \times n}$ die Gleichung
\\[0.2cm]
\hspace*{1.3cm}
$\texttt{det}(A) \cdot \texttt{det}(B) = \texttt{det}(A \cdot B)$
\\[0.2cm]
richtig ist.
\vspace*{0.2cm}

\noindent
\textbf{Hinweis}:  Zeigen Sie die
Behauptung zunächst für den Fall, dass $A$ eine Elementar-Matrix ist und nutzen Sie aus, dass eine
invertierbare Matrix als Produkt von Elementar-Matrizen dargestellt werden kann.
\eoxs

\begin{Satz}
  Für eine $n \times n$ Matrix $A \in \mathbb{K}^{n \times n}$ gilt
  \\[0.2cm]
  \hspace*{1.3cm}
  $\texttt{det}(A) = 0$ \quad g.d.w. \quad $A$ ist nicht invertierbar.
\end{Satz}

\proof
Wir zerlegen den Beweis dieses Satzes in zwei Teile.
\begin{enumerate}
\item[``$\Rightarrow$'']
  
     Es sei als $\texttt{det}(A) = 0$.  Wäre $A$ invertierbar, so würde nach dem eben bewiesenen
     Satz folgen, dass $\texttt{det}(A) \not= 0$ wäre.  Folglich kann $A$ nicht invertierbar sein.
\item[``$\Leftarrow$'']

     Nun sei $A$ nicht invertierbar.  Dann gilt $\mathtt{Kern}(A) \not= \{\vec{0}\}$.  Also gibt
     es einen Vektor $\vec{x}$, so dass $A \cdot \vec{x} = \vec{0}$ ist.  Schreiben wir
     $A$ als
     \\[0.2cm]
     \hspace*{1.3cm}
     $A = (\vec{a}_1, \cdots, \vec{a}_n)$,
     \\[0.2cm]
     wobei $\vec{a}_1,\cdots,\vec{a}_n$ die  Spalten-Vektoren von $A$ sind, so folgt aus
     der Gleichung $A \cdot \vec{x} = \vec{0}$ die Gleichung
     \\[0.2cm]
     \hspace*{1.3cm}
     $\ds\sum\limits_{i=1}^n x_i \cdot \vec{a}_i = \vec{0}$,
     \\[0.2cm]
     wobei hier die $x_i$ die Komponenten des Vektors $\vec{x}$ bezeichnen.  Damit sind aber die
     Spalten-Vektoren der Matrix $A$ linear abhängig und nach einem früher bewiesenen Lemma folgt
     die Gleichung
     $\texttt{det}(A) = 0$. \qed
\end{enumerate}

\exercise  
Zeigen Sie, dass die Gleichung  
\\[0.2cm] 
\hspace*{1.3cm}
$\texttt{det}(A) \cdot \texttt{det}(B) = \texttt{det}(A \cdot B)$ 
\\[0.2cm] 
für beliebige Matrizen  $A, B \in \mathbb{K}^{n \times n}$ gilt.  
\eoxs  
 
\exercise
Eine Matrix $A = (a_{i,j}) \in \mathbb{K}^{n \times n}$ ist eine \emph{\color{blue}obere Dreiecks-Matrix} genau dann, wenn alle Einträge der
Matrix unterhalb der Diagonalen den Wert $0$ haben, formal gilt
\\[0.2cm]
\hspace*{1.3cm}
$\forall i,j \in \{1,\cdots,n\}: i > j \rightarrow a_{i,j} = 0$.
\\[0.2cm]
Zeigen Sie, dass für eine obere Dreiecks-Matrix $A = (a_{i,j}) \in \mathbb{K}^{n \times n}$ die
Determinante nach der Formel
\\[0.2cm]
\hspace*{1.3cm}
$\texttt{det}(A) = \prod\limits_{i=1}^n a_{i,i}$
\\[0.2cm]
berechnet werden kann.  
\vspace*{0.2cm}

\noindent
\textbf{Hinweis}:  Der Beweis der Behauptung kann am einfachsten unter Verwendung der Definition
der Determinante nach Leibniz erbracht werden. 
\eoxs

\remark
Die in der letzten Aufgabe angegebene Formel zur Berechnung der Determinante einer oberen
Dreiecks-Matrix liefert ein effizientes Verfahren um die Determinante einer beliebigen Matrix $A$ zu
berechnen:  Zunächst wird die Matrix $A$ durch die Multiplikation mit geeigneten Elementar-Matrizen
zu einer oberen Dreiecks-Matrix $D$ umgeformt.  Wenn dabei nur Elementar-Matrizen der Form
$\mathrm{ZA}_n(k,l,\alpha)$ und $\mathrm{ZP}_n(k,l)$ verwendet werden, dann hat die Matrix $D$ bis
auf das Vorzeichen  dieselbe Determinante wie die Matrix $A$ und die Determinante von $D$ lässt
sich als Produkt der Diagonal-Elemente der Matrix $D$ berechnen.  

\begin{figure}[!ht]
\centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  firstnumber   = 1,
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.8cm,
                  xrightmargin  = 0.8cm,
                ]
    determinant := procedure(A) {
        n := #A;              
        sign := 1;
        for (i in [1 .. n]) {
            r := pivot(A, n, i);
            if (r != i) {
                [ A[i], A[r] ] := [ A[r], A[i] ];   
                sign := -sign;
            }
            if (A[i][i] == 0) {
                return 0;
            }
            for (k in [i+1 .. n]) {
                f := A[k][i]/A[i][i];
                for (j in [i .. n]) {
                    A[k][j] -= f * A[i][j];
                }
            }    
        }
        return sign */ [ A[i][i] : i in {1 .. n} ];
    };
\end{Verbatim}
\vspace*{-0.3cm}
\caption{Berechnung der Determinante.}
\label{fig:determinant-algorithm.stlx}
\end{figure}

Abbildung zeigt ein \href{http://randoom.org/Software/SetlX}{\textsc{SetlX}}-Programm, das den oben
beschriebenen Algorithmus zur Berechnung umsetzt.  Wir diskutieren es Zeile für Zeile.
\begin{enumerate}
\item Die Matrix \texttt{A}, deren Determinante berechnet werden soll, wird als Liste ihrer Zeilen
      dargestellt.  Das ist dieselbe Darstellung, die wir auch bei dem in Abbildung
      \ref{fig:inverse.stlx} auf Seite \pageref{fig:inverse.stlx} gezeigten Programm zur Berechnung
      des \blue{Inversen} einer Matrix verwendet haben.
\item \texttt{n} ist die Anzahl der Zeilen der Matrix.
\item In der Variablen \texttt{sign} merken wir uns, wie oft wir bei der Berechnung der
      Determinanten Zeilen vertauscht haben.  Solange diese Anzahl gerade ist, hat \texttt{sign} den
      Wert $+1$, aber wenn die Anzahl ungerade ist, hat \texttt{sign} den Wert $-1$.
\item Die Schleife in Zeile 4 des Programms hat die Aufgabe, die Elemente von \texttt{A}, die in der Spalte
      unter dem Element \texttt{A[i][i]} stehen, durch Addition geeignete Vielfacher der
      \texttt{i}-ten Zeile zu $0$ zu reduzieren.
\item Aus Gründen der numerischen Stabilität des Verfahrens ist es sinnvoll, zunächst die Zeile
      \texttt{r} zu bestimmen, für die der Absolutbetrag \texttt{A[r][i]} maximal wird.  Die
      Berechnung dieser Zeile geschieht mit Hilfe der Funktion \texttt{pivot}, deren Implementierung
      in Abbildung \ref{fig:inverse.stlx-pivot} gezeigt ist.  Wir hatten diese Funktion bereits bei
      der Berechnung des Inversen der Matrix $A$ benutzt.
\item Falls wir tatsächlich eine Vertauschung von Zeilen vornehmen, falls also $\mathtt{r} \not= \mathtt{i}$ ist,
      drehen wir das Vorzeichen \texttt{sign} um.
\item Sollte \texttt{A[i][i]} den Wert $0$ haben, so brauchen wir nicht weitermachen, denn wenn ein
      Element auf der Diagonalen einer Dreiecks-Matrix $0$ ist, ist natürlich auch das Produkt aller
      Diagonal-Elemente $0$.  Wir geben daher in diesem Fall in Zeile 11 den Wert $0$ zurück.
\item Ansonsten ziehen wir nun das 
      \\[0.2cm]
      \hspace*{1.3cm}
      $\ds\frac{\texttt{A[k][i]}}{\texttt{A[i][i]}}$-fache der $i$-ten Zeile von der $k$-ten Zeile ab,
      \\[0.2cm]
      so dass danach \texttt{A[k][i]} den Wert $0$ hat.
\item Am Ende berechnen wir das Produkt aller Diagonal-Elemente und multiplizieren es noch mit dem
      Vorzeichen \texttt{sign}, denn dies ist die gesuchte Determinante der Matrix \texttt{A}.
      \eoxs
\end{enumerate}

\exercise
Berechnen Sie die Determinante der Matrix
\\[0.2cm]
\hspace*{1.3cm}
$A := \left(
  \begin{array}{llll}
    1 & 2 & 3 & 4 \\
    2 & 3 & 4 & 1 \\ 
    3 & 4 & 1 & 2 \\ 
    4 & 1 & 2 & 3 \\ 
  \end{array}
\right)
$
\\[0.2cm]
mit dem Algorithmus, der in dem in Abbildung \ref{fig:determinant-algorithm.stlx} gezeigten
Programm implementiert ist.
\exend

\remark
In \textsc{SetlX} kann die Determinante einer Matrix nativ mit Hilfe der Funktion \texttt{la\_det} berechnet
werden.  Definieren wir beispielsweise die Matrix \texttt{A} als
\\[0.2cm]
\hspace*{1.3cm}
\texttt{A := << <<1 2 3>> <<2 1 3>> <<3 2 1>> >>;}
\\[0.2cm]
so können wir die Determinante dieser Matrix durch den Ausdruck
\\[0.2cm]
\hspace*{1.3cm}
\texttt{la\_det(A)}
\\[0.2cm]
berechnen und erhalten als Ergebnis $12.0$.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "lineare-algebra"
%%% ispell-local-dictionary: "deutsch8"
%%% End: 
